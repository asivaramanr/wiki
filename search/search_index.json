{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation as Code","text":""},{"location":"#introduction","title":"Introduction","text":"<p>With nowadays virtualization technologies, low latency communications, CPU Power and The Cloud, the Infrastructure paradigm is being changed from the static old-fashion way of managing servers to a new standard automation way of deploying services.</p> <p>Sysadmins are becoming SysDevs and the widely famous DevOps term is starting to be defined by itself. Of course, the old-fashion way is still there though \u2013 and always from my humble point of view \u2013 any company with growth objectives will need to start changing the way of doing things in less than five years and start to automate their infrastructure.</p>"},{"location":"#definition-still-a-new-concept","title":"Definition \u2013 Still a new concept?","text":"<p>When typing \u201cInfrastructure as Code\u201d in any Search Provider, about 350K results will be returned including a Wikipedia entry. New books are being published about this subject and it is clear that there is a new tendency when deploying new servers and services.</p> <p>There are several approaches when talking about code development techniques and documentation like Wikis, Git Markdown, documentation along with the source, etc.</p> <p>But what about doing it altogether? What about doing Documentation as Code?</p> <p>Documentation as Code is the way of automating documentation along with the written code. It can be when deploying new Infrastructure elements, new services or just a new piece of software or script. Automated Documentation will be able to be exported and shared using external tools like Wikis, Content Management, Git Pages or others.</p>"},{"location":"#the-idea","title":"The Idea","text":""},{"location":"#bash-example","title":"Bash Example","text":"<p>Let\u2019s suppose that we want to create a bash script. Our documentation will be together with our script code. The difference is that it will be able to be exported with a flag given. This flag will also allow the exporting of the document in markdown and even in HTML or PDF.</p> <pre><code>#!/usr/bin/env bash\n# Example of Documentation as Code\n\nif [ $# -eq 0 ]; then\n echo \"$(basename $0) [run|doc]\"\nfi\n\ncase $1 in\n run)\n\necho \"Here goes the code to run\"\n exit\n ;;\n\ndoc)\n\nsed '/^:/,/^DAC/!d;s/^:/cat/' \"$0\" | bash -s \"$@\"\n exit\n ;;\n\n*) exit\n ;;\nesac\n\n\n: &lt;&lt;DACv1\n## Documentation Starts\n\nHere you can add all the needed markdown documentation. You can split it as shown in this script.\nDACv1\n\n: &lt;&lt;\\DACv2\n### Documentation Continues\nHere goes the documentation example with a table:\n\n1 | 2 | 3 | 4 | 5\n---|---|---|---\nT | H | I | S |\nI | S |   | A |\nT | A | B | L | E\nDACv2\n</code></pre>"},{"location":"#python-example","title":"Python Example","text":"<p>Going on with the same approach, we can do the same with other scripting languages. Here it is my idea with Python.</p> <pre><code>#!/usr/bin/env python\n# Example of Documentation as Code\nimport optparse\n\ndef main():\n    parser = optparse.OptionParser(usage='Usage: %prog -o [run|doc]')\n    dac_choices = ['run', 'doc', 'Run', 'Doc', 'RUN', 'DOC'] \n    parser.add_option(\"-o\", dest=\"runordoc\",\n                      type=\"choice\",\n                      choices=dac_choices,\n                      help=\"Please specify if you want Run or Doc\")\n\n    (opts, args) = parser.parse_args()\n\n    if opts.runordoc is None:\n        print \"A mandatory option is missing\\n\"\n        parser.print_help()\n        exit(-1)\n\n    if opts.runordoc == \"Run\" or opts.runordoc == \"run\" or opts.runordoc ==\"RUN\":\n        print \"Here goes the code to run\"\n        exit(0)\n\n    elif opts.runordoc == \"Doc\" or opts.runordoc == \"doc\" or opts.runordoc ==\"DOC\":\n        DACv1 = \"\"\"## Documentation Starts\n\nHere you can add all the needed markdown documentation. You can split it as shown in this script.\n\n\"\"\"\n\n        DACv2 = \"\"\"### Documentation Continues\n\nHere goes the documentation example with a table:\n\n 1 | 2 | 3 | 4 | 5\n---|---|---|---\n T | H | I | S |\n I | S |   | A |\n T | A | B | L | E\n\"\"\"\n\n        print DACv1\n        print DACv2.strip()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"#powershell-example","title":"PowerShell Example","text":"<p>The \u201cHere-Strings\u201d in PowerShell will do the job. Let\u2019s see how it works.</p> <pre><code># Example of Documentation as Code\n\n[CmdletBinding(SupportsShouldProcess=$True)]\nParam(\n  [Parameter(Mandatory=$true,Position=0, HelpMessage=\"Please specify if you want Run or Doc\")]\n  [ValidateSet(\"Run\",\"Doc\")]\n  [string]$RunorDoc = $null\n)\n\nif ($PSCmdlet.ShouldProcess(\"$RunorDoc\",\"Return Options\"))\n\n{\nswitch ($RunorDoc)\n{\n    \"Run\" {Write-Output \"Here goes the code to run `$RunorDoc = $RunorDoc\"}\n\n    \"Doc\" {\n\n# DACv1    \n\n@\"\n## Documentation Starts\n\nHere you can add all the needed markdown documentation. You can split it as shown in this script.\n\n\"@\n\n# DACv2\n\n@\"\n\n### Documentation Continues\n\nHere goes the documentation example with a table:\n\n1 | 2 | 3 | 4 | 5\n---|---|---|---\nT | H | I | S |\nI | S |   | A |\nT | A | B | L | E\n\"@\n                }\n        }\n}\n</code></pre>"},{"location":"#final-step","title":"Final Step","text":"<p>From here, we can use tools like Pandoc or AsciiDoctor inside our scripts to automatically convert and post our documentation to a Wiki, GitHub, Content Management, etc. Another approach is keeping the markdown format and using Jekyll to automatically creating amazing documentation portals.</p>"},{"location":"#conclusion","title":"Conclusion","text":"<p>Documentation as Code is still a raw concept and I\u2019m sure that it will start to emerge in a few years or months. To me, it is a procedure together with tools to get the best documentation and revision control along with the code. The way of creating documentation must be simple and the result must be useful.  The scripts shown here are just ideas and I\u2019m sure that once you read this, a lot of new ones will strike your head.</p> <p>Note</p> <p>These docs are created for my personal use. You are welcome to read them.</p>"},{"location":"AWS/0200_EC2_Overview/","title":"Creating a EC2 Instance","text":""},{"location":"AWS/0200_EC2_Overview/#ec2-components-overview","title":"EC2 Components Overview.","text":"<p>EC2 instances created from images called Amazon Machine Images. Contains info like CPU, memory, storage, etc. and OS type.</p> <p>Initial login via key pairs, either generated or previously provided public (ONLY) key.</p> <p>Linux: SSH</p> <p>Windows: RDP</p> <p>EC2 Firewall config. EC2 instances associated with Security Group and the Security Groups can have network traffic policies applied to them.</p> <p>Virtual Private Cloud (VPC) is an isolated network. There is always one Default VPC but additional ones can be added. Each VPC has its own (isolated) subnet ranges and network ACLs on ingress/egress points.</p> <p>Elastic IP addresses are linked to an AWS account, not any particular VM instance. These can be mapped to VM instances (one-to-one) via NAT. Can have up to 5 EIPs per VPC. This makes VMs visible to the Internet.</p> <p>Elastic Block Storage is persistent block-based (not file/object based) storage. Linked to specific VMs. Can be encrypted.</p> <p>Instance Store: temp storage used while instance is running.</p> <p>S3: object storage</p>"},{"location":"AWS/0200_EC2_Overview/#ec2-concepts","title":"EC2 Concepts.","text":"<p>AMIs are used to create VM instances and define its resources.</p> <p>Amazon limits the total on-demand instances per region. On-demand instances are charged by the hour. This is contrasted with a \"reserved instance\" which is paid for an extended period of time (1+ year?)</p> <p>\"spot instance\" auction for vm instances other customers aren't using. We specify a bid price and when that matches or exceeds a spot price, we would gain access to the instance.</p> <p>EC2 instance types</p> <ul> <li>T: General purpose (software development)</li> <li>C: Compute-optimized (batch processing, analytics)</li> <li>R: Memory-optimized (databases)</li> <li>I or D: Storage-optimized (transactional database, data warehouse)</li> <li>G: GPU instances: Video encoding</li> </ul> <p>Further broken down into sub-types with specific details.</p> <p>AMIs can be custom or marketplace AMIs.</p> <p>We can add tags as key/value pairs to various resources. Tag keys and values are case-sensitive and scope is limited to within the same AWS account.</p> <p>Cannot tag IPs.</p>"},{"location":"AWS/0200_EC2_Overview/#creating-a-vpc","title":"Creating a VPC","text":"<p>One default VPC with every account. Analagous to multiple VLANs on one LAN.</p> <p>GUI VPC config wizard with common configs like \"VPC with a single public subnet\" or \"VPC with a private subnet only and hardware VPN access\"</p> <p>VPC configured with:</p> <ul> <li>name: unique identifer for the VPC</li> <li>CIDR block: the /16 or smaller block that all IPs within the VPC will use (this can be further divided later)</li> <li>Tenancy: Default or dedicated hardware</li> </ul> <p><code>DHCP options set</code> means DHCP is configured with the given options, which by default are an Amazon <code>.compute.internal</code> DNS domain and Amazon-provided DNS servers. The IP ranges provided by DHCP depend on the subnets which aren't yet present.</p> <p>Each VPC can have one or more subnets.</p> <p>Subnets configured with:</p> <ul> <li>Name tag: unique identifier for the VPC</li> <li>VPC: the VPC the subnet will exist within</li> <li>Availability zone: (not usually specified)</li> <li>CIDR block: the IP range for the subnet</li> </ul> <p>VM instances get linked to a particular subnet in a VPC.</p> <p>When a VM instance is created we have these configuration options:</p> <ul> <li>Number of instances</li> <li>Purchasing option: (this is where Spot Instances would be used)</li> <li>Network: our VPC network (NOT subnet)</li> <li>Subnet: our new subnet</li> <li>Auto-assign public IP:</li> <li>IAM role:</li> <li>Shutdown behavior:</li> <li>Enable termination protection: The Sarah Connor option?</li> </ul> <p>Private DNS auto-assigned, private IP also assigned.</p> <p>Check VPC again and now DHCP options like the DNS hostnames can be configured.</p> <p>Network ACL was auto-created with the VPC. Rule 100 allows all traffic. Default denies all traffic but won't get hit on this default, very open, config.</p> <p>Creation order is:</p> <ol> <li>Choose AMI</li> <li>Choose Instance Type</li> <li>Configure Instance</li> <li>Add Storage</li> <li>Tag instance</li> <li>Configure Security Group</li> </ol>"},{"location":"AWS/0200_EC2_Overview/#creating-a-linux-ec2-instance","title":"Creating a Linux EC2 Instance","text":"<p>Services -&gt; EC2 -&gt; Launch Instance</p> <p>When a VM instance is created we have these configuration options:</p> <ul> <li>Number of instances</li> <li>Purchasing option: (this is where Spot Instances would be used)</li> <li>Network: our VPC network (NOT subnet)</li> <li>Subnet: our new subnet</li> <li>Auto-assign public IP:</li> <li>IAM role: Allows AWS to manage key-based access</li> <li>Shutdown behavior: Stop or Terminate (auto-delete-self)</li> <li>Enable termination protection: often a good idea</li> <li>Monitoring:</li> <li>Tenancy:</li> </ul> <p>Adding storage-- default for Root already there. Delete on termination-- removes EBS when the instance is deleted.</p> <p>Tags: key/value pairs</p> <p>Security Group controls network access into the instance. SSH from anywhere enabled by default.</p> <p>SSH private key (PEM) must be safeguarded to ensure access to the instances.</p>"},{"location":"AWS/0200_EC2_Overview/#aws-marketplace","title":"AWS Marketplace","text":"<p>Useful for finding specialized appliances.</p> <p>LDAP? EVIL! :-)</p> <p>Check the VPC settings-- may not be what we want.</p> <p>The example e-commerce AMI includes a security group with port 80 and 443 open. (Which makes sense!)</p>"},{"location":"AWS/0200_EC2_Overview/#creating-a-windows-ec2-instance","title":"Creating a Windows EC2 Instance","text":"<p>New config item:</p> <ul> <li>Domain join directory: Joins a given AD domain</li> </ul> <p>RDP on port 3389 open by default. Uses public/private keys for auth but it sure seems a lot more complicated than the ssh-right-in approach Linux uses. Key to get a password to embed in a config file?</p>"},{"location":"AWS/0200_EC2_Overview/#connecting-to-an-ec2-instance","title":"Connecting to an EC2 Instance","text":"<p>The EC2 instance list has a \"connect\" tab which lets you download the RDP file describing the connection to the instance. The \"Get Password\" button offers an option to show the password, but it's encrypted.</p> <p>The <code>Key Pair Path</code> is the path to the PRIVATE key. Putting a private key in a web form seems like a bad idea in general and it's sad that this is common on AWS.</p> <p>The PEM secret key needs to be (locally) converted to use PuTTY. Save without passphrase? Bad example! OK, well, at least he acknowledges the weakness. Maybe he needs to learn about <code>pageant.exe</code>.</p> <p>Normal linux username: <code>ec2-user</code></p>"},{"location":"AWS/0200_EC2_Overview/#security-groups","title":"Security Groups","text":""},{"location":"AWS/0200_EC2_Overview/#ec2-security-groups-overview","title":"EC2 Security Groups Overview.","text":"<p>Virtual firewalls to control access to EC2 instances.</p> <p>When the policy for a security group is changed, all instances in that group see the changes.</p> <p>EC2-Classic (older config) had a single network shared by AWS customers. Security groups would be in the same region as the VM instances. There were at most 500 security groups per instance and max 100 rules per group. Can't change on running instances.</p> <p>New method: EC2-VPC which uses those virtual private clouds to logically isolate each AWS customer. These rules can change on running instances. 5 groups per instance and 50 rules per security group.</p> <p>Security groups control inbound and outbound network traffic. Outbound is allowed by default.</p> <p>For ICMP the type would be defined (vs. port)</p> <p>Default security group created for default VPC. Only allows inbound traffic from the default VPC, but allows all outbound.</p> <p>A custom security group requires a name and description.</p>"},{"location":"AWS/0200_EC2_Overview/#creating-and-deleting-an-ec2-security-group","title":"Creating and Deleting an EC2 Security Group.","text":"<p>Services -&gt; EC2 -&gt; Network &amp; Security -&gt; Security Groups</p> <p>(Launches a Windows instance where RDP is inaccessible by Security Group rules. Odd.)</p> <p>Ah... he adds the rule later.</p>"},{"location":"AWS/0200_EC2_Overview/#adding-rules-to-an-ec2-security-group","title":"Adding Rules to an EC2 Security Group.","text":"<p>Services -&gt; EC2 -&gt; Network &amp; Security -&gt; Security Groups</p> <p>Custom TCP: opens up port range as an option</p> <p>All traffic: might be useful to allow instances in different VPCs to intercommunicate.</p> <p>\"My IP\" literally your current IP as seen on the Internet.</p>"},{"location":"AWS/0200_EC2_Overview/#configuring-ec2-security-groups-for-vpc","title":"Configuring EC2 Security Groups for VPC.","text":"<p>Services -&gt; EC2 -&gt; Network &amp; Security -&gt; Security Groups</p> <p>OR</p> <p>Services -&gt; VPC -&gt; Security -&gt; Security Groups</p> <p>Network ACLs might look like Security Groups but they apply to a subnet, not a VM. Security groups have no <code>DENY</code> option, but Network ACLs do.</p> <p>The source can be another security group-- allows traffic from another VPC. Source can also be an IP address range, e.g. part of another VPC.</p>"},{"location":"AWS/0200_EC2_Overview/#elastic-block-store-ebs","title":"Elastic Block Store (EBS)","text":""},{"location":"AWS/0200_EC2_Overview/#elastic-block-store-overview","title":"Elastic Block Store Overview.","text":"<p>Works like an unformatted hard drive for EC2 instances.</p> <p>High Availability replicates EBS within an availability zone automaticaly.</p> <p>Persistent storage even when instances are shut down.</p> <p>Encryption possible provided the instance supports it. (Seems odd-- if the encryption is within EBS itself that should be transparent to the OS on the instance.)</p> <p>EBS Types:</p> <ul> <li>Magnetic volume: 100 IOPS, 1TB max size</li> <li>General purpose: SSD, 3 IOPS/GB + 3000 IOPS burst, 16TB max size</li> <li>Provisioned IOPS: SSD, up to 20,000 IOPS and 320MB/s, 16TB max size</li> </ul> <p>EBS snapshots, can create new EBS volumes from snapshot.</p> <p>Unencrypted snapshots can be shared.</p> <p>Shapshots must stay in the same region where they were created.</p> <p>EBS encrypted volumes, each gets unique AES256 key. Managed via AWS, FIPS-approved (US government standard). Encrypted with a volume key and then an account key. Both would be needed to decrypt.</p> <p>To encrypt data, move it to an encrypted volume.</p> <p>Creation process Services -&gt; EC2 -&gt; Elastic Block Storage -&gt; Volumes -&gt; Create Volume</p> <p>Creation options:</p> <ul> <li>Type: e.g. magnetic, provisioned IOPS</li> <li>Size: in GB</li> <li>IOPS: (if provisioned IOPS)</li> <li>Availability Zone: keep this the same zone as the VMs which will be using it</li> <li>Snapshot ID:</li> <li>Encryption:</li> </ul>"},{"location":"AWS/0200_EC2_Overview/#creating-a-ebs-volumes","title":"Creating a EBS Volumes.","text":"<p>EBS can exist independent from any VM instance.</p> <p>Normally used for disk volumes within an instance that get frequent access.</p> <p>Services -&gt; EC2 -&gt; Elastic Block Storage -&gt; Volumes</p> <p>Selecting an existing volume shows what instance it's attached to under \"Attachment information.\"</p> <p>Snapshot ID: source snapshot for creating the new volume (copy of snapshot)</p> <p>Shows up as a new blank hard drive (Virtual SCSI device).</p>"},{"location":"AWS/0200_EC2_Overview/#creating-ec2-snapshots","title":"Creating EC2 Snapshots.","text":"<p>Snapshots hold changed blocks (incremental)</p> <p>EBS -&gt; Volumes -&gt; Actions -&gt; Create Snapshot</p> <p>If possible, shut down instance before creating snapshot of the root EBS volume.</p> <p>Creating can take hours for large EBS volumes.</p> <p>Can create a new EBS volume from a snapshot.</p>"},{"location":"AWS/0200_EC2_Overview/#ec2-amazon-machine-images-ami","title":"EC2 Amazon Machine Images (AMI)","text":""},{"location":"AWS/0200_EC2_Overview/#amazon-machine-images-overview","title":"Amazon Machine Images Overview.","text":"<p>Create new instances in EC2 from AMIs. The AMI defines a template for the root volume (e.g. base OS?), defines which accounts can use the AMI, and maps EBS to block devices within the instance OS image.</p>"},{"location":"AWS/0200_EC2_Overview/#ami-life-cycle","title":"AMI Life Cycle.","text":"<p>Create an AMI</p> <p>Use AMI to create an instance.</p> <p>Modify instance if needed and \"register\" to create a new AMI. These can also be \"deregistered\" to remove them from future use.</p> <p>The new AMi can also be used to launch new instances or copied directly to a new, independent AMI.</p> <p>Shared AMIs are available. Sharing can be public or to specific AWS accounts.</p> <p>We don't know how a public/shared AMI was created so we should use them with caution.</p> <p>Amazon-created AMIs are named \"amazon\"</p> <p>AMIs can be purchased or sold. Marketplace has categories and search.</p> <p>What actions appear on the <code>Actions</code> menu for an AMI?</p>"},{"location":"AWS/0200_EC2_Overview/#creating-an-ami-in-ec2","title":"Creating an AMI in EC2.","text":"<p>\"What if we find ourselves making the same changes once we launch an instance from an AMI?\" Me: It doesn't matter since those changes are all documented and automatically applied using modern Infrastructure as Code methods, which allow for easy, versioned, granular changes rather than baking all the changes into one giant, hard-to-adjust blob.</p> <p>Me: Creating AMIs for known-source-traceability seems like a much better reason for making an AMI.</p> <p>EC2 -&gt; Instances -&gt; Actions -&gt; Image -&gt; Create Image</p> <p>Can create from an EBS snapshot.</p> <p>RHN subscriptions not maintained through snapshots? That seems odd-- I would expect it to be duplicated since the machine registration is stored on the EBS volume. Perhaps they mean that RedHat detects the duplication and disables one or both of the subscriptions?</p> <p>Windows can also have problem when starting from a snapshot.</p> <p>Advise that it's best to create an AMI from an instance rather than a snapshot.</p> <p>Copy AMI copies to another region.</p>"},{"location":"AWS/0200_EC2_Overview/#importing-and-exporting-an-ami-in-ec2","title":"Importing and Exporting an AMI in EC2.","text":"<p>Import can allow us to re-use an existing virtual machine (hosted at Amazon.)</p> <p>Also can be used for DR.</p> <p>Managed via CLI or API.</p> <p>Preparing the source VM requires things like:</p> <ul> <li>Remove security software (antivirus / IDS)</li> <li>Disconnect virtual DVD-ROMs since backing storage probably doesn't exist in AWS</li> <li>Reconfigure for DHCP (optional, but allows immediate instance access that way.)</li> <li>Remove VMware tools or other hypervisor-specific software</li> <li>Sysprep (Windows) or equivalent (RHEL) (optional)</li> </ul> <p>Sounds more like making a template than moving a VM...</p> <p>Export it from existing environment, make an OVF.</p> <p>Upload to AWS via <code>aws ec2 import-image --cli-input-json ...</code></p> <p>(Based on the test, I had that wrong-- the upload precedes the import but I didn't notice how they did it in the video.)</p> <p>Export from EC2 via <code>aws ec2 create-instance-export-task</code>.</p>"},{"location":"AWS/0200_EC2_Overview/#raid-on-ebs","title":"RAID on EBS","text":""},{"location":"AWS/0200_EC2_Overview/#ebs-raid-configuration-options","title":"EBS RAID Configuration Options.","text":"<p>Software RAID within the OS.</p> <p>(This seems like a terrible idea for AWS images. Isn't the EBS already redundant/reliable? We should avoid making our instances individually tough and instead focus on making them easily replaced.)</p> <p>RAID-0 might have a place if we have a single image that requires a truly ridiculous amount of storage throughput. The Measured IOPS caps at 20,000 / 320MB/s per EBS. Would we really be able to get more with software RAID-0 or would other internal limit be hit? (e.g. virtual SCSI limits.)</p> <p>I think that there are probably better solutions for the problems software RAID within AWS is trying to solve. (E.g. high performance databases.)</p> <p>Overall this doesn't seem AWS-specific and should probably be avoided on AWS as a general rule.</p>"},{"location":"AWS/0200_EC2_Overview/#creating-an-ebs-raid-array-on-linux","title":"Creating an EBS RAID Array on Linux.","text":"<p><code>fdisk</code> really? The Linux RAID auto is quite optional. Whole disks can be used for the <code>md</code> device, though we probably want to use one of the static <code>/dev/disk/by-id</code> devices or a <code>disk-mapper</code> device.</p> <p><code>mdadm --create</code> for creating new <code>md</code> device</p> <p><code>mdadm --detail</code> for info about the device</p> <p><code>mkfs</code> on bare device? It hurts! If you're going to all this trouble, at least use LVM. (Well, at least he didn't partition the <code>md</code> device...)</p>"},{"location":"AWS/0200_EC2_Overview/#creating-an-ebs-raid-array-on-windows","title":"Creating an EBS RAID Array on Windows.","text":"<p>Server Manager -&gt; Tools -&gt; Computer Management -&gt; Disk Management</p> <p>Right click new disk -&gt; New Mirrored Volume</p> <p>Add both disks to \"selected\", choose drive letter</p> <p>Add a label. Convert to dynamic disks.</p>"},{"location":"AWS/0200_EC2_Overview/#monitoring","title":"Monitoring","text":""},{"location":"AWS/0200_EC2_Overview/#configuring-ec2-load-balancers","title":"Configuring EC2 Load Balancers.","text":"<p>Sits between our service instances (e.g. web servers) and the Internet and spreads the load across the service instances. Users connect to the load balancers.</p> <p>Process:</p> <ol> <li>Define Load Balancer</li> <li>Assign Security Groups</li> <li>Configure Security Settings</li> <li>Configure Health Check</li> <li>Add EC2 Instances</li> <li>Add Tags</li> </ol> <p>Define load balancer: needs to know the protocol + port for the incoming requests (\"load balancer protocol\") and the same for your back-end instances.</p> <p>The load balancer subnets need to be in at least two different availability zones. This is not the instance back-end subnet, but the subnets for the front-end connections end users will be directed into. (They look to be NATted so there's something else going on too.)</p> <p>Create security group to allow appropriate access. (e.g. TCP port 80 from anywhere.)</p> <p>Health checks: make sure back end instances are healthy via HTTP GET. (Would it be useful to have a combination of frequent \"simple\" health checks and infrequent \"complex\" health checks?)</p> <p>We should create a DNS record (CNAME) to the load balancer DNS name. (That can be slow since it requires two DNS lookups from separate domains to complete. Is there a better way?) Route53 allows an \"Alias\" A record to be created directly to the ELB: Route 53 docs</p>"},{"location":"AWS/0200_EC2_Overview/#performing-ec2-health-checks","title":"Performing EC2 Health Checks.","text":"<p>Is an instance running properly or not?</p> <p>Demo on how to filter by state when viewing instances in the web interface.</p> <p>\"Monitoring\" has some per-instance performance metrics and history.</p> <p>Auto-scaling lets us auto-create instances in response to monitoring events.</p> <p>Seems like a big emphasis on instance resource monitors rather than measuring application health. Probably because it's simple and easy, not because it's useful. \"The light is better over here.\"</p> <p>EC2 Actions possible:</p> <ul> <li>Rebooot instance: stays on same underlying hardware</li> <li>Recover instance: moves instance image to new hardware</li> <li>Stop instance</li> <li>Terminate instance (delete)</li> </ul>"},{"location":"AWS/0200_EC2_Overview/#monitoring-ec2-instances-with-cloudwatch","title":"Monitoring EC2 Instances with CloudWatch.","text":"<p>Allow for tracking and alerting of metrics on resources.</p> <p>The metric graphs could be helpful for troubleshooting a specific performance issue, but really are best used in conjunction with application-level information and metrics. E.g. What is the application waiting for when performance is poor?</p> <p>Defining alarms: \"consecutive periods\" what is that? Defined at the lower right. \"period\"</p> <p>EC2 Actions could be useful (e.g. auto-reboot to clear memory leaks.)</p>"},{"location":"AWS/0200_EC2_Overview/#practice-configuring-ec2","title":"Practice: Configuring EC2.","text":""},{"location":"AWS/0200_EC2_Overview/#aws-cli-test","title":"AWS CLI test.","text":"<p>How to test credentials for AWS Command Line Tools</p> <p><code>aws sts get-caller-identity</code></p>"},{"location":"AWS/0200_EC2_Overview/#create-a-vpc-and-security-group","title":"Create a VPC and Security Group.","text":"<pre><code>describe vpc('ec2-overview-vpc') do\n  it { should exist }\nend\n\ndescribe security_group('ec2-overview-sg') do\n  it { should exist }\n  its(:outbound) { should be_opened }\n  its(:inbound) { should be_opened(22) }\nend\n</code></pre>"},{"location":"AWS/0200_EC2_Overview/#create-an-ami-from-an-ec2-instance","title":"Create an AMI from an EC2 instance.","text":"<pre><code>describe ec2('ec2-overview-ec2-01') do\n  it { should belong_to_vpc('ec2-overview-vpc') }\n  it { should have_security_group('ec2-overview-sg') }\nend\n\ndescribe ami('ec2-overview-ami') do\n  it { should exist }\nend\n</code></pre>"},{"location":"AWS/0200_EC2_Overview/#create-three-ebs-volumes","title":"Create three EBS volumes.","text":"<pre><code>describe ebs('ec2-overview-ebs01') do\n  it { should exist }\n  it { should be_attached_to('ec2-overview-ec2-01') }\nend\ndescribe ebs('ec2-overview-ebs02') do\n  it { should exist }\n  it { should be_attached_to('ec2-overview-ec2-01') }\nend\ndescribe ebs('ec2-overview-ebs02') do\n  it { should exist }\n  it { should be_attached_to('ec2-overview-ec2-01') }\nend\n</code></pre>"},{"location":"AWS/0300_EC2_Autoscaling/","title":"Autoscaling using EC2","text":"<p>Before looking at the overview of Autoscalling, lets configure the ASW-CLI to deploy our resources without accessing the console. </p>"},{"location":"AWS/0300_EC2_Autoscaling/#aws-command-line","title":"AWS Command Line","text":""},{"location":"AWS/0300_EC2_Autoscaling/#installing-the-aws-cli-on-windows","title":"Installing the AWS CLI on Windows.","text":"<p>Click here to download the latest AWSCLIV2.msi</p>"},{"location":"AWS/0300_EC2_Autoscaling/#installing-the-aws-cli-on-linux","title":"Installing the AWS CLI on Linux.","text":"<p>Amazon Linux AMI includes Amazon AWS CLI. Handy!</p> <p>Linux install doesn't use OS packaging.</p> <p>Also a \"pip\" based option which might be slightly better than a script-based install.</p> <p>AWS CLI needs Java 1.7.0 or later.</p> <p>Possibly helpful env vars:</p> <ul> <li><code>EC2_BASE</code>: install dir for the utilities</li> <li><code>EC2_HOME</code>: <code>$EC2_BASE/tools</code> (parent of the <code>bin</code> directory)</li> <li><code>AWS_ACCESS_KEY</code>: key ID used for CLI access</li> <li><code>AWS_SECRET_KEY</code>: secret key used for CLI access (security risk having it as ENV)</li> <li><code>EC2_URL</code>: includes region, e.g. <code>https://ec2.us-west-2.amazonaws.com</code></li> <li><code>PATH</code>: <code>$PATH:$EC2_HOME/bin</code></li> <li><code>JAVA_HOME</code>: path to Java install location</li> </ul> <p>Verify setup via <code>ec2-describe-regions</code> or <code>ec2-describe-instances</code></p>"},{"location":"AWS/0300_EC2_Autoscaling/#configuring-the-aws-cli","title":"Configuring the AWS CLI.","text":"<p><pre><code>aws configure\n</code></pre> It will ask for below information:</p> <ul> <li><code>AWS Access Key ID [****************EAFY]</code>:</li> <li><code>AWS Secret Access Key [****************5m37]</code>:</li> <li><code>Default region name [US West (Oregon)]: us-west-2</code>:</li> <li><code>Default output format [None]: json</code></li> </ul> <p>Use your own Key ID and Secret Access Key, preferably for a non-root account.</p> <p><code>aws ec2 help</code>: more info about EC2 commands</p> <p><code>aws ec2 describe-instances</code>: lots of instance details</p>"},{"location":"AWS/0300_EC2_Autoscaling/#creating-an-iam-role-for-ec2-with-aws-cli","title":"Creating an IAM Role for EC2 with AWS CLI.","text":"<p>Create a JSON file describing the role, a trust policy file.</p> <p><code>aws iam create-role --role-name &lt;name&gt; --assume-role-policy-document file://filename.json</code></p> <p>This might also be helpful for creating policies (mentioned in another course):</p> <pre><code>\"--generate-cli-skeleton\" (string) Prints a JSON skeleton to standard\noutput without sending an API request. If provided with no value or\nthe value \"input\", prints a sample input JSON that can be used as an\nargument for \"--cli-input-json\". If provided with the value \"output\",\nit validates the command inputs and returns a sample output JSON for\nthat command.\n</code></pre> <p>Example:</p> <pre><code>$ aws iam create-role --generate-cli-skeleton\n{\n    \"Path\": \"\",\n    \"RoleName\": \"\",\n    \"AssumeRolePolicyDocument\": \"\",\n    \"Description\": \"\",\n    \"MaxSessionDuration\": 0,\n    \"PermissionsBoundary\": \"\",\n    \"Tags\": [\n        {\n            \"Key\": \"\",\n            \"Value\": \"\"\n        }\n    ]\n}\n</code></pre> <p><code>aws iam list-roles</code>: shows all our roles</p>"},{"location":"AWS/0300_EC2_Autoscaling/#configuring-an-iam-role-for-ec2-with-aws-cli","title":"Configuring an IAM Role for EC2 with AWS CLI.","text":"<p>Show permission policies assigned to a role with <code>aws iam list-attached-role-policies --role-name &lt;name&gt;</code></p> <p>Example <code>aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess --role-name WebApp</code></p>"},{"location":"AWS/0300_EC2_Autoscaling/#launching-an-instance-with-an-iam-role-using-aws-cli","title":"Launching an Instance with an IAM Role Using AWS CLI.","text":"<p>Instance Profile: group for storing roles</p> <p>Example: <code>aws iam add-role-to-instance-profile --instance-profile-name s3readonly --role-name WebApp</code></p> <p>In the GUI on step 3 of configuring an instance, there's the option for \"IAM role\" to use.</p> <p><code>aws ec2 run-instances --image-id ami-&lt;stuff&gt; --count 1 --key-name &lt;key pair name&gt; --instance-type t2.micro --iam-instance-profile Arn=arn:aws:iam:&lt;account id&gt;:instance-profile/&lt;instance profile name&gt;</code></p> <p>The permissions in the policy will be applied to software running within this instance, for example the <code>aws</code> CLI.</p> <p>Above CLI created a nameless instance. Correct that with:</p> <p><code>aws ec2 create-tags --resources &lt;ID&gt; --tags Key=Name,Value=&lt;instance name&gt;</code></p>"},{"location":"AWS/0300_EC2_Autoscaling/#boot-scripts","title":"Boot Scripts","text":""},{"location":"AWS/0300_EC2_Autoscaling/#writing-shell-scripts","title":"Writing Shell Scripts.","text":"<p>When creating instance:</p> <p>Configure Instance -&gt; Advanced Details -&gt; User Data</p> <p>Add bash commands to run including shebang line. (Terrible example of allow world-write to a directory in his example.)</p> <p>(Don't need sudo to read /etc/passwd.)</p>"},{"location":"AWS/0300_EC2_Autoscaling/#configuring-the-cloud-init-directive","title":"Configuring the <code>cloud-init</code> Directive.","text":"<p>Configure Instance -&gt; Advanced Details -&gt; User Data</p> <pre><code>#cloud-config\nrepo_update: true\nrepo_upgrade: all\n\npackages:\n - httpd\nruncmd:\n - chkconfig httpd on\n - service httpd start\n</code></pre> <p>Hope that RHEL7 server isn't using systemd... oh, wait... it does!</p> <p>Should instead be:</p> <pre><code>runcmd:\n - systemctl enable httpd\n - systemctl start httpd\n</code></pre> <p>Or:</p> <pre><code>runcmd:\n - systemctl enable --now httpd\n</code></pre>"},{"location":"AWS/0300_EC2_Autoscaling/#auto-scaling-groups","title":"Auto Scaling Groups","text":""},{"location":"AWS/0300_EC2_Autoscaling/#planning-ec2-auto-scaling","title":"Planning EC2 Auto Scaling.","text":"<p>Launch Configuration: configuration of the instances used for scaling up</p> <p>Scaling plans: defines when to scale and how to scale</p> <p>Launch config set up when the group is initially created. Describes how the auto-added instances will be configured.</p> <p>Only one launch configuration per auto scaling group. Launch configuration cannot be changed. (Destroy and re-create the scaling group instead?) Looks like a new Launch Configuration would be created and the new Configuration assigned to the Auto Scaling Group.</p> <p>Launch config contains:</p> <ul> <li>AMI to use</li> <li>Instance type</li> <li>Other instance config items to use when new instances are created</li> </ul> <p>Scaling plan can do things like:</p> <ul> <li>Maintain a given number of instances</li> <li>Re-create instances to replace unhealthy ones</li> <li>Manual or automatic creation</li> <li>Schedule-based scaling</li> <li>Demand-based scaling (E.g. instance CPU usage.)</li> </ul>"},{"location":"AWS/0300_EC2_Autoscaling/#creating-a-launch-configuration","title":"Creating a Launch Configuration.","text":"<p>Can create a Launch Configuration from an existing instance using EC2 -&gt; Instances -&gt; Actions -&gt; Attach to Auto Scaling Group</p> <p>EC2 -&gt; Auto Scaling -&gt; Launch Configurations -&gt; Create Auto Scaling Group first screen is \"Create a new launch configuration\" vs. \"Create an Auto Scaling group from an existing launch configuration\"</p>"},{"location":"AWS/0300_EC2_Autoscaling/#creating-a-launch-configuration-from-an-instance","title":"Creating a Launch Configuration from an Instance.","text":"<p>EC2 -&gt; Instances -&gt; Actions -&gt; Attach to Auto Scaling Group</p> <p>Prompted to create a new Auto Scaling group or change the Launch Configuration for an existing Auto Scaling group. (I bet the latter would make a heck of a mess if done unexpectedly.)</p> <p>EC2 -&gt; Auto Scaling -&gt; Launch Configurations shows the config copied from the instance we used above.</p> <p>Also have a \"Copy Launch Configuration\" button to create yet another launch configuration based on this one which was based on an instance.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#creating-an-auto-scaling-group","title":"Creating an Auto Scaling Group.","text":"<p>Groups of healthy servers that serve up identical content.</p> <p>EC2 -&gt; Auto Scaling -&gt; Auto Scaling Groups -&gt; \"Create Auto Scaling Group\"</p> <p>Steps to create a Launch Configuration:</p> <ol> <li>Choose AMI</li> <li>Choose Instance Type</li> <li>Configure details</li> <li>Add Storage</li> <li>Configure Security Group</li> <li>Review</li> </ol> <p>Steps to create an Auto Scaling Group:</p> <ol> <li>Configure Auto Scaling group details</li> <li>Configure scaling policies</li> <li>Configure Notifications</li> <li>Configure Tags</li> </ol> <p>Group Details include</p> <ul> <li>Name</li> <li>Initial group size: (number of instances)</li> <li>Network to use</li> <li>Subnet(s) to use</li> <li>Advanced Details: Load Balancing -&gt; use an Elastic Load Balancer + health check details</li> </ul> <p>Scaling policies: either keep the group at its initial size or use scaling policies to adjust the capacity</p> <ul> <li>Scale between <code>initial size</code> and <code>max size</code> instances</li> <li>Increase group size:</li> <li>Name</li> <li>Execute policy when: (alarm definition)</li> <li>Take the action: Add (or Set to) <code>N</code> instances</li> <li>Decrease group size:</li> <li>Name</li> <li>Execute policy when: (alarm definition)</li> <li>Take the action: Remove <code>N</code> instances</li> </ul> <p>Putting the instances on a schedule is done using the AWS CLI.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#creating-an-auto-scaling-group-from-an-instance","title":"Creating an Auto Scaling Group from an Instance.","text":"<p>EC2 -&gt; Instances -&gt; Actions -&gt; Attach to Auto Scaling Group</p> <p>New or existing Auto Scaling group.</p> <p>Instance cannot be part of another Auto Scaling group. Must be in the same Availability Zone as the Auto Scaling group. he AMI used to launch this instance must still be available.</p> <p>Scaling Policy created from new, including alarm creation.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#linking-an-instance-to-a-vpc","title":"Linking an Instance to a VPC.","text":"<p>EC2 -&gt; Launch Instance</p> <p>On step 3 (Instance Details) we can choose a VPC to contain the Instance.</p> <p>VPC defined when instance is launched, not editable. Can create an AMI then launch that AMI into a new VPC.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#tagging-auto-scaling-groups","title":"Tagging Auto Scaling Groups.","text":"<p>Auto Scaling group tags can be auto-added to the new instances created from scaling actions.</p> <p><code>aws autoscaling create-or-update-tags --tags</code> via CLI.</p> <p><code>aws autoscaling describe-tags</code></p> <p>Up to 10 tags can be added per Auto Scaling group.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#configuring-auto-scaling-groups","title":"Configuring Auto Scaling Groups","text":""},{"location":"AWS/0300_EC2_Autoscaling/#load-balancing-an-auto-scaling-group","title":"Load Balancing an Auto Scaling Group.","text":"<p>Having a method to spread load across a group that varies in size makes sense-- that's how the extra capacity gets used.</p> <p>Want to be sure we're using a Launch Configuration using an AMI with our website content already included.</p> <p>Health Check type can be EC2 or ELB.</p> <p>\"OutOfService\" could mean the servers are broken or it might just be that the checks haven't completed yet.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#attaching-an-instance-to-an-auto-scaling-group","title":"Attaching an Instance to an Auto Scaling Group.","text":"<p>Can't attach more than the max instance to an Auto Scaling Group.</p> <p>EC2 -&gt; Instances -&gt; Actions -&gt; Instance Settings -&gt; Attach to Auto Scaling Group</p>"},{"location":"AWS/0300_EC2_Autoscaling/#detaching-an-instance-from-an-auto-scaling-group","title":"Detaching an Instance from an Auto Scaling Group.","text":"<p>EC2 -&gt; Instances -&gt; Actions -&gt; MISSING</p> <p>EC2 -&gt; Auto Scaling -&gt; Instances -&gt; Checkbox -&gt; Lower Actions -&gt; Detach</p>"},{"location":"AWS/0300_EC2_Autoscaling/#suspending-and-resuming-an-auto-scaling-group","title":"Suspending and Resuming an Auto Scaling Group.","text":"<p>The instances stay running, but the automated actions like launching new instances, terminating old instances, and performing health checks will not take place while suspended.</p> <p>No web GUI method to do this-- CLI only.</p> <p><code>aws autoscaling suspend-process --auto-scaling-group-name</code></p> <p><code>aws autoscaling resume-process --auto-scaling-group-name</code></p>"},{"location":"AWS/0300_EC2_Autoscaling/#shutting-down-an-auto-scaling-group","title":"Shutting Down an Auto Scaling Group.","text":"<p>Shut down = delete. Permanent.</p> <p>EC2 -&gt; Auto Scaling -&gt; Actions -&gt; Delete</p> <p>Launch Configurations remain behind. Load Balancer also not deleted. Instances will be terminated (deleted) when the group is deleted</p>"},{"location":"AWS/0300_EC2_Autoscaling/#placement-groups","title":"Placement Groups","text":""},{"location":"AWS/0300_EC2_Autoscaling/#overview-of-placement-groups","title":"Overview of Placement Groups.","text":"<p>Group of instances within a single Availability Zone.</p> <p>Enable 10Gig low-latency network connectivity between specialized \"enhanced networking\" instances. The instance OS must also support this type of \"enhanced networking.\"</p> <p>Placement groups cannot be merged later on.</p> <p>Placement group created first. Then instances are launched within the placement group. Use the same instance type within the group. Existing instances cannot be moved to a placement group. (But an AMI could be created from an existing instance and that AMI used to launch new instances within the group.)</p> <p>Instance types allowed in a placement group:</p> <ul> <li>General Purpose</li> <li>m4.large through m4.10xlarge</li> <li>Compute Optimized</li> <li>c4.large through c4.8xlarge</li> <li>c3.large through c3.8xlarge</li> <li>cc2.8xlarge</li> <li>Memory Optimized</li> <li>cr1.8xlarge</li> <li>r3.large - r3.8xlarge</li> <li>Storage Optimized</li> <li>d2.large - d2.8xlarge</li> <li>hi1.4xlarge</li> <li>hs1.4xlarge</li> <li>i2.xlarge - i2.8xlarge</li> <li>GPU Optimizaed</li> <li>cg1.4xlarge</li> <li>g2.2xlarge - g2.8xlarge</li> </ul>"},{"location":"AWS/0300_EC2_Autoscaling/#creating-a-placement-group","title":"Creating a Placement Group.","text":"<p>EC2 -&gt; Network &amp; Security -&gt; Placement Groups</p> <p>Only parameter: account-wide unique name for the Placement Group</p> <p>Strategy: \"cluster\" (what does that mean?)</p> <p>Choosing an instance type during creation that can't go into the placement group means the \"Placement Group\" option in the \"Configure Instance\" step won't appear.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#launching-an-instance-into-a-placement-group","title":"Launching an Instance into a Placement Group.","text":"<p>(If a \"moderate\" network performance instance like c4.large is put in a placement group does that performance change?)</p> <p>Provided the right instance type is chosen we have the option to put it into a placement group during the \"Configure Instance\" step.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#deleting-a-placement-group","title":"Deleting a Placement Group.","text":"<p>Can't delete a placement group that contains instances.</p> <p>Actions -&gt; instance state -&gt; terminate</p> <p>Placement Groups -&gt; Delete Placement Group</p>"},{"location":"AWS/0300_EC2_Autoscaling/#practice-auto-scaling","title":"Practice: Auto Scaling","text":""},{"location":"AWS/0300_EC2_Autoscaling/#create-a-lunch-configuration","title":"Create a Lunch Configuration.","text":"<p>Services -&gt; EC2 -&gt; Auto Scaling -&gt; Launch Configurations -&gt; Create launch configuration</p> <p>Steps:</p> <ol> <li>Choose AMI</li> </ol> <p>Using \"Amazon Linux 2 AMI\" (ami-0cb72367e98845d43)</p> <ol> <li>Choose Instance Type</li> </ol> <p>t2.micro (keep it free!)</p> <ol> <li>Configure details</li> </ol> <p>name: <code>ec2-autoscaling-launch-config</code>    * (others left at defaults)</p> <ol> <li>Add Storage</li> </ol> <p>Using <code>snap-05f1f2daed90efa18</code> by default, presumably part of the Amazon AMI. Leave default, including \"delete on termination\" since auto-created and auto-terminated instances are treated as disposable.</p> <ol> <li> <p>Configure Security Group</p> </li> <li> <p>Assign a security group: Create a new security group</p> </li> <li>Security group name: <code>ec2-autoscaling-sg</code></li> <li>Description: <code>For training autoscaling exercise</code></li> <li>Type: SSH</li> <li> <p>Source: My IP</p> </li> <li> <p>Review</p> </li> <li> <p>Create a new key pair</p> </li> <li>Key pair name <code>ec2-autoscaling-keypair</code></li> <li>Use PuTTYgen to convert to OpenSSH format with passphrase</li> <li>Add key to \"pageant\" so you can stop typing the passphrase all the time</li> </ol> <p>NOTE: It looks like Amazon wants us to use Launch Templates instead.</p> <pre><code>Defining a launch template instead of a launch configuration allows you to have multiple versions of a template. With versioning, you can create a subset of the full set of parameters and then reuse it to create other templates or template versions. For example, you can create a default template that defines common configuration parameters such as tags or network configurations, and allow the other parameters to be specified as part of another version of the same template.\n\nWe recommend that you use launch templates instead of launch configurations to ensure that you can use the latest features of Amazon EC2, such as T2 Unlimited instances.\n</code></pre>"},{"location":"AWS/0300_EC2_Autoscaling/#create-a-launch-template","title":"Create a Launch Template.","text":"<p>Beforehand:</p> <ol> <li>Create Security Groups</li> <li><code>ec2-autoscaling-web-sg</code> for HTTP/HTTPS only from anywhere</li> <li><code>ec2-autoscaling-sg</code> for SSH only + HTTP/HTTPS from above <code>ec2-autoscaling-web-sg</code> web security group to allow ELB traffic into the instances.Type <code>sg</code> into the source field to bring up a drop-down of security groups. Magic!</li> <li>Create Key Pair <code>ec2-autoscaling-keypair</code></li> <li>Create Network Interfaces <code>ec2-autoscaling-nic01</code></li> <li>Subnet: picked the one on us-west-2a</li> <li>Security Group: <code>ec2-autoscaling-sg</code></li> <li>Copy/paste ID for below config <code>eni-0897f5a5b47381d07</code></li> <li>This didn't seem to be used yet I was prompted to create one when it didn't exist. Odd.</li> </ol> <p>Services -&gt; EC2 -&gt; Instances -&gt; Launch Templates -&gt; Create Launch Template</p> <p>Create launch template</p> <ul> <li>Launch template name: <code>ec2-autoscaling-launch-template</code></li> <li>Template version description: <code>initial version</code></li> <li>Source template: None (default)</li> <li>AMI ID: <code>ami-0cb72367e98845d43</code> (found via \"Search for AMI\")</li> <li>Instance type: <code>t2.micro</code> (selection doesn't show free tier as easily.)</li> <li>Key pair name: <code>ec2-autoscaling-keypair</code></li> <li>Security Groups: <code>ec2-autoscaling-sg</code> (doesn't seem to have as easy an option for creating new Security Groups)</li> <li>Network Interfaces: (lots of off-screen horizontal issues here with horizontal scrolling only possible when vertically scrolled to the bottom)</li> <li>Provided we already created a network interface this section doesn't appear to be required.</li> <li>This seems like something that should be handled with sensible defaults on auto-created instances.</li> </ul> <p>TODO: AMI doesn't have a webserver running by default. Need to enable it or the ELB health check fails.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#edit-launch-template","title":"Edit Launch Template.","text":"<p>Add this to <code>User Data</code>:</p> <pre><code>#!/bin/sh\nyum -y install httpd\nsystemctl enable --now httpd\necho '&lt;h1&gt;Your Autoscaling lab exercise worked!&lt;/h1&gt;' &gt; /var/www/html/index.html\n</code></pre>"},{"location":"AWS/0300_EC2_Autoscaling/#edit-auto-scaling-group","title":"Edit Auto Scaling Group.","text":"<p>Change the Launch Template version to <code>$Latest</code></p>"},{"location":"AWS/0300_EC2_Autoscaling/#create-an-auto-scaling-group","title":"Create an Auto Scaling Group.","text":"<p>Services -&gt; EC2 -&gt; Auto Scaling -&gt; Auto Scaling Groups</p> <p>Choose <code>ec2-autoscaling-launch-template</code></p> <ol> <li> <p>Configure Auto Scaling group details</p> </li> <li> <p>Group name: <code>ec2-autoscaling-group</code></p> </li> <li> <p>Subnet: (use the us-west-2a default)</p> </li> <li> <p>Configure scaling policies</p> </li> <li> <p>Keep this group at its initial size (we'll set up scaling later on to avoid a chicken-and-egg problem with setting up the Elastic Load Balancer.)</p> </li> <li> <p>Configure Notifications</p> </li> <li> <p>Add notification</p> </li> <li>Send notification to <code>ec2-autoscaling-notify</code></li> <li>With these recipients: <code>&lt;add E-mail addresses here&gt;</code></li> <li> <p>Whenever instances: (leave default)</p> </li> <li> <p>Configure Tags</p> </li> <li> <p>No tags needed for this exercise</p> </li> <li> <p>Perhaps a name tag could have been useful?</p> </li> <li> <p>Review</p> </li> </ol>"},{"location":"AWS/0300_EC2_Autoscaling/#attach-instances","title":"Attach Instances.","text":"<p>Services -&gt; EC2 -&gt; Instances</p> <p>Already one instance running since the Auto Scaling Group auto-creates one instance to start.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#load-balance-the-auto-scaling-group","title":"Load Balance the auto scaling group.","text":"<p>Services -&gt; EC2 -&gt; Load Balancers -&gt; Application Load Balancer -&gt; Create</p> <ol> <li>Configure Load Balancer</li> </ol> <p>1.1 Basic Configuration</p> <ul> <li>Name: <code>ec2-autoscaling-elb</code></li> <li>Scheme: internet-facing (default)</li> <li>IP address type: IPv4 (default)</li> </ul> <p>1.2 Listeners</p> <ul> <li>HTTP: port 80 (default)</li> </ul> <p>1.3 Availability Zones</p> <ul> <li>VPC: (use default)</li> <li> <p>Availability Zones:</p> <ul> <li><code>us-west-2a</code> IPv4 address assigned by AWS (default)</li> <li><code>us-west-2b</code> IPv4 address assigned by AWS (default)</li> </ul> </li> <li> <p>Configure Security Settings</p> </li> </ul> <p>(I would need to use a registered domain in order to make use of Amazon AWS Certificate Manager)</p> <ol> <li>Configure Security Groups</li> </ol> <p>Use <code>ec2-autoscaling-web-sg</code> to allow HTTP/HTTPS</p> <ol> <li> <p>Configure Routing</p> </li> <li> <p>Target group: new target group</p> </li> <li> <p>Name: <code>ec2-autoscaling-elb-target</code></p> </li> <li> <p>Register Targets</p> </li> <li> <p>Add the one instance running from the autoscaling auto-creation</p> </li> <li> <p>Review</p> </li> </ol> <p>Services -&gt; EC2 -&gt; Auto Scaling -&gt; Auto Scaling Groups -&gt; Edit</p> <ul> <li>Target Groups: <code>ec2-autoscaling-elb-target</code></li> <li>Health Check Type: ELB</li> </ul>"},{"location":"AWS/0300_EC2_Autoscaling/#optional-change-scaling-policy-to-use-elb","title":"(Optional) Change scaling policy to use ELB.","text":"<ul> <li>Use scaling policies to adjust the capacity of this group</li> <li>Scale between 1 and <code>4</code> instances.</li> <li>Name: <code>ec2-autoscaling-load-policy</code></li> <li>Metric type: <code>Application load balancer request count per target</code></li> <li>Target value: <code>1000</code> (this is arbitrary, in a real deployment this would be determined by measuring application performance as connections went up.)</li> <li>Instance need: <code>300</code> seconds to warm up after scaling (also arbitrary, but slamming an instance that's still booting isn't going to give good performance results.)</li> </ul>"},{"location":"AWS/0300_EC2_Autoscaling/#security-group-checks","title":"security group checks.","text":"<p><code>awspec generate security_group &lt;VPC ID&gt;</code></p> <p>This ends up with group IDs hardcoded. I changed the ELB security group to <code>:ELB_security_group</code> and my source IP + '/32' to <code>:admin_source_IP</code> in the below example. The group ID in the second group should match the <code>:ELB_security_group</code>. There's probably a way to tell <code>awspec</code> how to to do that automatically, but I don't yet know how.</p> <pre><code>describe security_group('ec2-autoscaling-sg') do\n  let(:admin_source_IP) { '1.2.3.4/32' }\n  let(:ELB_security_group) { 'sg-0aeb724c8e35d7d91' }\n\n  it { should exist }\n  its(:group_name) { should eq 'ec2-autoscaling-sg' }\n  its(:inbound) { should be_opened(80).protocol('tcp').for(:ELB_security_group) }\n  its(:inbound) { should be_opened(22).protocol('tcp').for(:admin_source_IP) }\n  its(:inbound) { should be_opened(443).protocol('tcp').for(:ELB_security_group) }\n  its(:outbound) { should be_opened.protocol('all').for('0.0.0.0/0') }\n  its(:inbound_rule_count) { should eq 3 }\n  its(:outbound_rule_count) { should eq 1 }\n  its(:inbound_permissions_count) { should eq 3 }\n  its(:outbound_permissions_count) { should eq 1 }\nend\n\ndescribe security_group('ec2-autoscaling-web-sg') do\n  it { should exist }\n  its(:group_id) { should eq 'sg-0aeb724c8e35d7d91' }\n  its(:group_name) { should eq 'ec2-autoscaling-web-sg' }\n  its(:inbound) { should be_opened(80).protocol('tcp').for('0.0.0.0/0') }\n  its(:inbound) { should be_opened(443).protocol('tcp').for('0.0.0.0/0') }\n  its(:outbound) { should be_opened.protocol('all').for('0.0.0.0/0') }\n  its(:inbound_rule_count) { should eq 2 }\n  its(:outbound_rule_count) { should eq 1 }\n  its(:inbound_permissions_count) { should eq 2 }\n  its(:outbound_permissions_count) { should eq 1 }\nend\n</code></pre>"},{"location":"AWS/0300_EC2_Autoscaling/#autoscaling-group-checks","title":"autoscaling group checks.","text":"<p><code>awspec generate autoscaling_group &lt;VPC ID&gt;</code></p> <p>The subnet IDs won't translate to other environments, but are probably worth keeping in place for easy changing to new VPCs.</p> <pre><code>describe autoscaling_group('ec2-autoscaling-group') do\n  it { should exist }\n  its(:auto_scaling_group_name) { should eq 'ec2-autoscaling-group' }\n  its(:min_size) { should eq 1 }\n  its(:max_size) { should eq 4 }\n  its(:desired_capacity) { should eq 1 }\n  its(:default_cooldown) { should eq 300 }\n  its(:availability_zones) { should eq [\"us-west-2a\", \"us-west-2b\"] }\n  its(:health_check_type) { should eq 'ELB' }\n  its(:health_check_grace_period) { should eq 300 }\n  its(:vpc_zone_identifier) { should eq 'subnet-4ac1c633,subnet-af8cade4' }\n  its(:termination_policies) { should eq [\"Default\"] }\n  its(:new_instances_protected_from_scale_in) { should eq false }\n  its('launch_template.launch_template_name') { should eq 'ec2-autoscaling-launch-template' }\n  it { should have_alb_target_group('ec2-autoscaling-elb-target') }\nend\n</code></pre>"},{"location":"AWS/0300_EC2_Autoscaling/#elastic-load-balancing-checks","title":"Elastic Load Balancing checks.","text":"<p><code>$ awspec generate elb &lt;VPC ID&gt;</code></p> <p>No output produced. Odd.</p> <p><code>$ aws elb describe-load-balancers</code> has no net output:</p> <pre><code>{\n    \"LoadBalancerDescriptions\": []\n}\n</code></pre> <p>However, that's because we're using ELBv2 (newer):</p> <pre><code>$ aws elbv2 describe-load-balancers\n{\n    \"LoadBalancers\": [\n        {\n            \"LoadBalancerArn\": \"arn:aws:elasticloadbalancing:us-west-2:146679865661:loadbalancer/app/ec2-autoscaling-elb/e3b9918dc5f21a15\",\n            \"DNSName\": \"ec2-autoscaling-elb-1546742660.us-west-2.elb.amazonaws.com\",\n            \"CanonicalHostedZoneId\": \"Z1H1FL5HABSF5\",\n            \"CreatedTime\": \"2019-06-04T18:11:27.940Z\",\n            \"LoadBalancerName\": \"ec2-autoscaling-elb\",\n            \"Scheme\": \"internet-facing\",\n            \"VpcId\": \"&lt;VPC ID&gt;\",\n            \"State\": {\n                \"Code\": \"active\"\n            },\n            \"Type\": \"application\",\n            \"AvailabilityZones\": [\n                {\n                    \"ZoneName\": \"us-west-2b\",\n                    \"SubnetId\": \"subnet-4ac1c633\"\n                },\n                {\n                    \"ZoneName\": \"us-west-2a\",\n                    \"SubnetId\": \"subnet-af8cade4\"\n                }\n            ],\n            \"SecurityGroups\": [\n                \"sg-0aeb724c8e35d7d91\"\n            ],\n            \"IpAddressType\": \"ipv4\"\n        }\n    ]\n}\n</code></pre> <p>Maybe it's just a documentation issue?</p> <pre><code>$ awspec generate elbv2 vpc-5468c22c\nCould not find command \"elbv2\".\nDid you mean?  \"elb\"\n</code></pre> <p>Nope.</p> <p>It looks like ELBv2 goes by the alias \"ALB\" as seen in this awspec github issue.</p> <p><code>awspec generate alb &lt;VPC ID&gt;</code></p> <p>```bash describe alb('ec2-autoscaling-elb') do   it { should exist }   its(:load_balancer_name) { should eq 'ec2-autoscaling-elb' }   its(:scheme) { should eq 'internet-facing' }   its(:type) { should eq 'application' }   its(:ip_address_type) { should eq 'ipv4' } end ````</p> <p>There it is! Interesting that the security group info isn't there.</p>"},{"location":"AWS/0300_EC2_Autoscaling/#final-check-go-to-the-elb-public-ip-and-confirm-we-see-the-apache-page","title":"Final Check: Go to the ELB public IP and confirm we see the Apache page","text":"<p>Success shows \"Your Autoscaling lab exercise worked!\"</p> <p>Failures might include:</p> <ul> <li> <p>503: Service Unavailable</p> </li> <li> <p>When no targets on the back end</p> </li> <li> <p>504: Gateway Time-out</p> </li> <li> <p>Immediately after targets added to ELB</p> </li> <li>Instance security group doesn't allow ELB to reach port 80</li> </ul>"},{"location":"AWS/0400_Databases_and_Application_Services/","title":"Databases and Application Services","text":""},{"location":"AWS/0400_Databases_and_Application_Services/#relational-databases","title":"Relational Databases","text":""},{"location":"AWS/0400_Databases_and_Application_Services/#relational-database-service-overview","title":"Relational Database Service Overview.","text":"<p>RDS cloud instances can run MS SQL, Oracle, PostgreSQL, MySQL, or Amazon Aurora.</p> <p>Steps for deploying an RDS:</p> <ol> <li>Select Engine</li> <li>Production?</li> <li>Specify DB Details: like instance size/class, multi-availability-zone config, underlying storage type, etc.</li> <li>Configure Advanced Settings</li> </ol> <p>Storage 5GB to 3TB. Different EBS volume types available. 30,000 IOPS max. (20,000 for other types?)</p> <p>VPC (or EC2?) security groups can limit access to the database so only appropriate sources will work.</p> <p>EC2 security groups with databases are deprecated.</p> <p>Multi-AZ = synchronous replication across the AZs. Can also generate read-only replicas in other AZs.</p> <p>DB parameter groups define the database engine configuration. They can apply to DB instances of the same type (e.g. MySQL)</p> <p>DB option groups applied to a specific instance to support vendor-specific items (Oracle, MS SQL Server, and MySQL only.) For example enabling network encryption.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#launching-an-rds-db-instance","title":"Launching an RDS DB Instance.","text":"<p>Launching a new instance can be done in a few minutes. In many cases we don't even need a usage license.</p> <p>SQL Server can be deployed with our own license or asking Amazon to use an included license.</p> <p>Advanced settings include things like:</p> <ul> <li>VPC to use</li> <li>Subnet</li> <li>Public access?</li> <li>Availability zone</li> <li>VPC security group</li> <li>Database port</li> <li>DB parameter group (?)</li> <li>Backup retention</li> <li>Backup window</li> <li>Auto-upgrade minor version</li> <li>Maintenance window for minor version updates</li> </ul> <p>Details:</p> <ul> <li>Endpoint: how other services connect to this database instance.</li> </ul> <p>Can use remote admin tools on the endpoint to do DBA tasks. Specify the user account we created with the DB instance.</p> <p>Import/Export or bulk copy can be used to bring data into a new instance.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#managing-rds-db-access-with-iam","title":"Managing RDS DB Access with IAM.","text":"<p>Can use DB-specific tools for user management. Users would be specific to that instance.</p> <p>Can attach policies to users/groups to give RDS privileges such as <code>AmazonRDSFullAccess</code> or <code>AmazonRDSReadOnlyAccess</code> or get much more fine-grained with a custom policy.</p> <p>Policy Generator:</p> <ul> <li>AWS Service: Amazon RDS</li> <li>Actions: (lots of options)</li> <li>ARN: * (for all instances)</li> <li>Conditions: might require MFA, for example</li> <li>Policy Name: probably want something more descriptive than the default</li> <li>Can Validate Policy, then Create Policy</li> </ul>"},{"location":"AWS/0400_Databases_and_Application_Services/#encrypting-rds-resources","title":"Encrypting RDS Resources.","text":"<p>Encryption applies to database instance, snapshots, backups, logs, and read-only replicas. Enabled during creation of the instance.</p> <p>Uses a default AWS account-wide key (if available) or an instance-specific key we specify. Key cannot be changed later</p> <p>Amazon Aurora cannot be encrypted. MS SQL, Oracle, MySQL, and PostgreSQL can be.</p> <p>All storage types are supported.</p> <p>Only these instance types can be encrypted:</p> <ul> <li>General purpose (M3): db.m3.medium - db.m3.2xlarge</li> <li>Memory optimized (R3): db.r3.large - db.r3.8xlarge</li> <li>Memory optimized (CR1): db.cr1.8xlarge</li> </ul> <p>Keys are managed by AWS Key Management Service.</p> <p>Default keys cannot be deleted, revoked, or rotated. Customer-managed keys can.</p> <p>CloudTrail can audit key usage.</p> <p>Existing instances cannot be encrypted. Encrypted instances cannot have encryption disabled. Unencrypted backups cannot be restored to an encrypted instance. (That seems nonsensical, I can understand the reverse...)</p> <p>KMS keys are assigned to a specific region so we cannot copy encrypted snapshots between regions, nor replicate encrypted instances between regions.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#rds-security-groups","title":"RDS Security Groups.","text":"<p>Two types of security groups with RDS</p> <ol> <li>DB security group (only on EC2-classic, deprecated)</li> <li>Each rule allows a source (IP, EC2 security group) access to a database</li> <li>inbound only</li> <li>Port numbers are inferred from the config</li> <li>VPC security group</li> <li>allows a source to access and instance</li> <li>source can be IP range or another VPC security group</li> <li>inbound or outbound</li> <li>Ports specified to match DB ports</li> </ol> <p>DB security groups can control access to instances not in the same VPC. They use the RDS API to set access. Don't require protocol/port info. They allow access from EC2 groups.</p> <p>VPC security groups work within the same VPC. They use EC2 APIs to set access. They use TCP and require port info.</p> <p>If the app servers + DB are in the same VPC, we can use the same VPC security group for both groups of EC2 instances.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#other-database-services","title":"Other Database Services","text":""},{"location":"AWS/0400_Databases_and_Application_Services/#dynamodb-concepts","title":"DynamoDB Concepts.","text":"<p>NoSQL database service-- non-relational database. Simple key-value store.</p> <p>Tables contain multiple items. Item = row = record = tuple.</p> <p>No limit to the items per table. Items can be added via GUI, or AWS SDK for Java, .NET, or PHP. (Seems like a small list.)</p> <p>Attributes = fact = column = field</p> <p>Primay key: uniquely identifies a particular row/item. Required for each row.</p> <p>Two kinds of primary keys:</p> <ol> <li> <p>Hash attribute: single attribute which uniquely identifies the record within a table</p> </li> <li> <p>Hash and range attributes: more than one item together which combine to uniquely identifies a record within a table</p> </li> </ol> <p>Secondary indexes: quickly find data on non-primary-key attributes. Once created, the index is accessed like a table.</p> <p>Creating a DynamoDB:</p> <ol> <li>Primary Key</li> <li>Table Name: specify name for the DB here</li> <li>Add Indexes (optional)</li> <li>Provisioned Throughput Capacity in read and write \"capacity units\"</li> <li>Additional options (optional)</li> <li>Enable streams: useful for replicating changes to DynamoDB to other databases</li> <li>Basic alarms: useful for determining if the usage is exceeding the available throughput</li> </ol>"},{"location":"AWS/0400_Databases_and_Application_Services/#working-with-dynamodb-local","title":"Working with DynamoDB Local.","text":"<p>Free of charge, does not interact with AWS' real DynamoDB. Useful for prototyping or testing apps that would use DynamoDB without the cost of an actual DynamoDB instance.</p> <p>Run as a Java JAR file like:</p> <p><code>java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb</code></p> <p>Web-based semi-GUI at <code>http://localhost:8000/shell/</code></p>"},{"location":"AWS/0400_Databases_and_Application_Services/#overview-of-aws-elasticache","title":"Overview of AWS ElastiCache.","text":"<p>Distributed in-memory cache of Database info.</p> <p>Based on <code>memcached</code> or <code>redis</code> engine.</p> <p>Redis allows the use of lists. Memcached is simpler and scales better.</p> <p>Cache nodes contain CPU/memory used for caching. Cache cluster = collection of cache nodes. Cache Parameter Groups are used to manage cache settings for each node. Cache Replication Groups define multiple copies of the data which can be accessed different ways.</p> <p>Security is whitelist, based on EC2 instance.</p> <p>Performance tuning parameters include heartbeat (delay between read synchronization?), number of databases, memory to reserve for non-cache use.</p> <p>Replication group: 2+ sychronized cluster nodes used for high availability</p> <p>Not all regions support ElastiCache.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#launching-an-elasticache-cluster","title":"Launching an ElastiCache cluster.","text":"<p>Configuring ElastiCache:</p> <ol> <li>Select Engine</li> <li>Memcached</li> <li>Redis</li> <li>Specify Cluster Details</li> <li>Engine version</li> <li>Port</li> <li>Parameter group</li> <li>Enable replication</li> <li>Cluster name</li> <li>Note type</li> <li>S3 location of Redis RDB file</li> <li>Configure Advanced Settings</li> <li>Cache subnet group</li> <li>Availability Zone</li> <li>VPC Security Group</li> <li>Automatic backups</li> <li>Maintenance Window</li> <li>Topic for SNS notifications</li> <li>Review</li> </ol> <p>Adjust security group to allow Redis port 6379 if needed.</p> <p>Endpoint for Redis is needed for clients to connect with it. (Is network-based access really faster? Shouldn't this be local to each EC2 node running the Redis client?)</p> <p>Created new EC2 instance in the same security group as ElastiCache used (or at least one that would allow communications with Redis' TCP port.)</p> <p>Test basic Redis functions using telnet to port 6379:</p> <p>(Note the lack of any authentication-- those security groups appear pretty important to data security!)</p> <p><code>telnet &lt;endpoint&gt; 6379</code></p> <p><code>info server</code></p> <p><code>info all</code></p> <p><code>exists customerid</code></p> <p><code>append customerid \"100\"</code></p> <p><code>get customerid</code></p> <p>(Our inserted data is present. Scary!)</p> <p>Some tiny amount of security is available in theory, but is very vulnerable to brute-force attacks: redis password protection</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#overview-of-aws-redshift","title":"Overview of AWS RedShift.","text":"<p>Data Warehouse service.</p> <p>To use:</p> <ol> <li>Provision a Redshift cluster</li> <li>Upload data sets</li> <li>Run analysis queries</li> </ol> <p>Redshift clusters have \"leaders\" which orchestrates the proces of receiving queries, splitting the load, gathering replies, and responding back to the client request.</p> <p>Clusters need only one node. Can add/remove online. Snapshots can be made.</p> <p>Permissions managed via IAM. Security groups define access from SQL client tools-- either RedShift-specific security groups or VPC security groups.</p> <p>Encryption possible.</p> <p>Redshift IAM policies include the usual defaults like <code>AmazonRedshiftFullAccess</code> and <code>AmazonRedshiftReadOnlyAccess</code>.</p> <p>Redshift monitoring</p> <ul> <li>Database audit logging (tracking access)</li> <li>Events and notifications when \"certain things\" happen</li> <li>Performance metrics (CloudWatch)</li> </ul> <p>Redshift creates one database by default but more can be added.</p> <p>Parameter groups are used to make consistent configurations across databases (?) in the same cluster. Or would that be used to create multiple clusters with the same config? (a bit unclear.)</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#big-data","title":"Big Data","text":""},{"location":"AWS/0400_Databases_and_Application_Services/#amazon-kinesis-overview","title":"Amazon Kinesis Overview.","text":"<p>Processes large amounts of data in real-time. Dat often processed as it's created. A Kinesis application is created from client libraries or APIs. It takes data from \"producers\", fold/spindle/mutiliates it, then puts it in \"streams.\"</p> <p>What is a \"producer\"? Log file, transactions on a web app, data from mobile devices, etc.</p> <p>Once data is in a stream it can be directed to dashboards, alerts, or sent to other AWS services for storage.</p> <p>Terminology:</p> <ul> <li>Data record: one data unit in Kinesis</li> <li>Streams: ordered sequence (group) of Data Records</li> <li>Producers: write data to streams</li> <li>EC2 instances</li> <li>End users' client devices</li> <li>Mobile devices</li> <li>Other servers</li> <li>Consumers: read data from streams</li> <li>EC2 instance apps</li> <li>Kinesis application: run on the shards within an EC2 instance</li> <li>Application Name: identifies the Kenesis app</li> <li>Shard: group of data records with unique ID (also a unit of capacity)</li> <li>Partition key: groups data within a stream by shard</li> <li>Sequence number: unique number for each data record</li> </ul> <p>Creating a stream:</p> <p>Services -&gt; Kenesis -&gt; Create Stream</p> <p>Stream name and number of \"shards\" (1MB/s write + 2MB/s read capacity) needed.</p> <p>Kinesis apps run within EC2 instances. Kinesis apps have a unique name used for failover/HA.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#amazon-elastic-mapreduce-overview","title":"Amazon Elastic MapReduce Overview.","text":"<p>Used for storing and processing very large amounts of data without managing the details of the underlying infrastructure.</p> <p>Also useful for moving large data volumes to/from S3 or databases.</p> <p>EMR requires the use of a custom application which is run within the EMR cluster.</p> <p>An EMR cluster is created from EC2 instances-- at least a master node and optional slave nodes. These are provisioned from an AMI with Hadoop pre-installed.</p> <p>EMR is scalable-- we can add nodes for times when more processing needed. Can auto-replace nodes with performance issues or other problems.</p> <p>Can use S3 buckets for storage.</p> <p>EMR can also use Spark or Presto with (instead of?) Hadoop.</p> <p>Within the cluster, nodes use HDFS cluster filesystem for sharing. Apps and data used by the cluster externally are stored in S3.</p> <p>Cluster resources managed via YARN which splits up tasks.</p> <p>Application languages supported: Hive, Pig, Spark SQL, Mlib, GraphX, and Spark Streaming.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#application-services","title":"Application Services","text":""},{"location":"AWS/0400_Databases_and_Application_Services/#simple-queue-service-overview","title":"Simple Queue Service Overview.","text":"<p>A queue is used to store information temporarily. Apps can push messages (info) into the queue and others can pull data from the queue. Often used for web applications. This helps keep web applications more loosely coupled with their dependents.</p> <p>Delivery (eventually) is guaranteed even if there are temporary problems. Once the problems are fixed the consumer can poll the queue again and receive its messages.</p> <p>FIFO is NOT guaranteed.</p> <p>Message sizes are variable. Configurable settings per queue (e.g. polling interval.) There can be multiple simultaneous readers and writers per queue.</p> <p>Delay queues possible-- consumers only see the messages appear a given time after they are added to the queue.</p> <p>Either IAM or SQS policies can be used to control access to queues.</p> <p>SQS message lifecycle:</p> <ol> <li>Sender puts a message on the queue</li> <li>Consumer retrieves the message, \"visibility timeout\" period starts. During this time the message is invisible to other consumers.</li> <li>Consumer processes the message and deletes it before the \"visibility timeout\" expires</li> </ol> <p>Services -&gt; SQS -&gt; Create New Queue</p> <p>Queue Settings:</p> <ul> <li>Queue name:</li> <li>Default visibility timeout: 30s</li> <li>Message retention period: 4d</li> <li>Maximum message size: 256KB</li> <li>Delivery delay: 0s</li> <li>Receive message wait time: 0s</li> </ul> <p>Dead Letter Queue Settings:</p> <ul> <li>Use Redrive Policy:</li> <li>Dead letter queue:</li> <li>Maximum receives:</li> </ul> <p>Once created, we can add permissions from the SQS web GUI.</p> <p>We can also send a message to the queue from there. (Unusual) Normally apps would send messages to each other.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#simple-workflow-service-overview","title":"Simple Workflow Service Overview.","text":"<p>Development tool for making distributed web apps. The web app components could be running on different EC2 instances.</p> <p>A process for running coordinated tasks on multiple servers is needed. SWF handles scheduling the tasks, their dependencies, and managing concurrent task execution.</p> <p>Workflows contain activities. Activities perform tasks. Tasks can either be synchronous (sequential) or asynchronous (parallel) with other tasks (?). He says \"job\" but that's not defined yet.</p> <p>An activity worker takes a task and performs that activity. These are built by developers.</p> <p>Decider: coordinates work within a task. This logic is defined by developers.</p> <p>The task logic (code) can be written in any langage. It doesn't need to run within AWS.</p> <p>Workflow execution:</p> <ol> <li>Create activity workers (developers)</li> <li>Create a decider (developers)</li> <li>Register the activities and workflow within SWF</li> <li>Start the workers and decider</li> <li>Start executing the whole workflow</li> <li>View executions in AWS console</li> </ol> <p>Workflow example: splitting, uploading, and re-combining a large media file sent to S3.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#simple-notification-service-overview","title":"Simple Notification Service Overview.","text":"<p>Send messages to \"subscriber endpoints\" (so far sounds suspiciously similar to Simple Queue Service.)</p> <p>What is a \"subscriber endpoint\"?</p> <ul> <li>application running somewhere else</li> <li>user on mobile device who has subscribed to notifications</li> <li>newsletter on a website</li> <li>etc.</li> </ul> <p>Uses a publisher (producer) - subscriber (consumer) model. (Still sounds a lot like Simple Queue Service.)</p> <p>Subscribers don't need to poll for messages. (OK, there's a difference!) The messages get pushed to an app that has already agreed to receive them.</p> <p>Possible subscriber types:</p> <ul> <li>Users (desktops, mobile devices)</li> <li>E-mail</li> <li>Amazon SQS</li> <li>Amazon Lambda</li> <li>HTTP (HTTPS) endpoint</li> <li>SMS (texting)</li> </ul> <p>SNS \"topics\" combine multiple recipients into one group and can contain multiple endpoint types.</p> <p>Crucial API calls:</p> <ol> <li>CreateTopic</li> <li>Subscribe</li> <li>Publish</li> </ol> <p>Mobile push can use protocols like ADM, APNS, Baidu, GCM, MPNS, or WNS. (Getting definitions for these acronyms seems difficult. APNS is probably Apple Push Notification Service, but many of the others are completely ambiguous.)</p> <p>Services -&gt; Mobile -&gt; SNS -&gt; Create new topic</p> <p>Can sbscribe programmatically or via GUI: Actions -&gt; Subscribe to topic</p> <p>Protocols used for that method:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>Email</li> <li>Email-JSON</li> <li>Amazon SQS</li> <li>Application</li> <li>AWS Lambda</li> </ul> <p>E-mail subscription requests send the user an E-mail with a confirmation. They need to click the link (with the right Amazon cookies present) to confirm the subscription.</p> <p>Services -&gt; Mobile -&gt; SNS -&gt; Create platform application</p> <ul> <li>Application Name</li> <li>Push Notification Platform</li> <li>Amazon Device Messaging (ADM) (TLA revealed!)</li> <li>Apple production</li> <li>Apple development</li> <li>Baidu Cloud Push for Android in China</li> <li>Google Cloud Messaging (GCM)</li> <li>Microsoft MPNS for Windows Phone 7+</li> <li>Microsoft WNS for Windows 8+ and Windows Phone 8.1+</li> <li>Client ID</li> <li>Client secret</li> </ul>"},{"location":"AWS/0400_Databases_and_Application_Services/#sending-e-mail-with-simple-e-mail-service","title":"Sending E-mail with Simple E-mail Service.","text":"<p>Outbound SMTP E-mail sending service. SPAM MACHINE!</p> <p>Developers can build in sending E-mails using API calls.</p> <p>Or a mail client can use the SMTP mail relay to send directly. (Perhaps from an EC2 instance? This doesn't make a lot of sense in terms of most end users' real mail clients-- e.g. Outlook.)</p> <p>Another option, configure our SMTP relay to itself relay messages via Amazon Simple E-mail Service. That seems especially odd since we are likely to have a mix of non-AWS-sourced E-mails mixed in there.</p> <p>Services -&gt; Application Services -&gt; SES</p> <p>SMTP settings shows server name. SMTP requires a username/password to authenticate before a message will be accepted. These SMTP users are managed through (big surprise!) IAM. Note that the SMTP username is not the same as the IAM user name (since the SMTP usernames probably need to be globally unique.)</p> <p>The user gets created with a policy called <code>AmazonSESSendingAccess</code></p> <p>In order to send E-mails either our account needs to undergo some additional scrutiny to get out of \"sandbox\" SMTP access or E-mail addresses must be individually verified to be willing to receive E-mails from this AWS account.</p> <p>Can also send SES messages via AMS CLI using JSON.</p> <p>Example <code>destination.json</code>:</p> <pre><code>{\n  \"ToAddresses\":  [\"user@domain.com\"],\n  \"CcAddresses\":  [],\n  \"BccAddresses\":  []\n}\n</code></pre> <p>Example <code>message.json</code>:</p> <pre><code>{\n  \"Subject\": {\n    \"Data\": \"Subject of Email Goes Here\"\n    \"Charset\": \"UTF-8\"\n  },\n  \"Body\": {\n    \"Text\": {\n      \"Data\": \"Body of message goes here.\"\n      \"Charset\": \"UTF-8\"\n    }\n  }\n}\n</code></pre> <p><code>aws ses send-mail --from &lt;from address&gt; --destination file://destination.json --message file://message.json</code></p> <p>Output will be message ID.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#transcoding-media-with-elastic-transcoder","title":"Transcoding Media with Elastic Transcoder.","text":"<p>Convert media files from one format to another to allow for a wider playback audience (at the expense of image quality.)</p> <p>Services -&gt; Application Services -&gt; Elastic Transcoder -&gt; Create a new Pipeline</p> <p>The pipeline defines the transcoding jobs that we want to execute.</p> <p>Configuration options:</p> <ul> <li>Pipeline name</li> <li>Input bucket: (existing S3 bucket for uploaded media)</li> <li>IAM role: auto-created on first pipeline</li> <li>S3 Bucket: (for output)</li> <li>Storage class: standard or reduced redundancy (the latter might be appropriate since we can always re-transcode the original media.)</li> <li>S3 Bucket: (for thumbnails)</li> <li>Notifications (optional) (via SNS Topic)</li> <li>On progressing</li> <li>On Warning</li> <li>On Completion</li> <li>On Error</li> <li>Encryption (optional)</li> </ul> <p>\"Presets\" define various options for video output. For example:</p> <ul> <li>Generic 1080p (mp4)</li> <li>Generic 720p (mp4)</li> <li>iPhone4S (mp4)</li> <li>iPod Touch (mp4)</li> <li>Apple TV 2G (mp4)</li> </ul> <p>\"Jobs\" shows existing jobs or allows us to create a new job.</p> <p>Create new job parameters/options:</p> <ul> <li>Pipeline</li> <li>Input Key (S3 Keys): media file already uploaded. (\"Key\" is an odd description for a media file...)</li> <li>Output Key Prefix: start of the name of transcoded files (optional)</li> <li>Decryption Parameters</li> <li>Output Preset</li> <li>Output Key: Output file name</li> <li>Encryption Parameters</li> <li>Available settings</li> <li>Clip</li> <li>Captions</li> <li>Additional output details (optional)</li> <li>Watermarks (optional)</li> <li>Additional Outputs (e.g. another preset)</li> <li>Override Detected Input Parameters (e.g. force interpretation as a specific frame rate, etc.)</li> </ul> <p>Initial status \"progressing\"</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#cloudsearch-overview","title":"CloudSearch Overview.","text":"<p>Search capability can be added to data collections. For example web pages, documents, blogs, etc.</p> <p>Can self-provision storage to hold indexes and self-scale as the search volume changes.</p> <p>Can be unstructured data, semi-structured, or structured. (Not sure how these are defined.)</p> <p>Each searchable item is described as a document and gets a unique ID. Documents uploaded as JSON or XML but the content can be PDF, MS-Office docs, CSV, HTML, etc. which gets converted to JSON or XML for indexing.</p> <p>Deleting content might leave its search index behind for a short time.</p> <p>Search indexes defined by fields within the uploaded documents. Each document field that will be indexed needs an index field. Fields have a type, category, and possibly other attributes. Data in the documents to be indexed must match the indexing configuration.</p> <p>\"Facets\" are used to refine and filter searches on other index fields. (like date info?) They give an example of letting a user search within a specific region.</p> <p>I don't really follow what \"Hits that share the same value in a facet\" means... What's a \"value in a facet\"?</p> <p>Creating a CloudSearch:</p> <ol> <li>Create a CloudSearch domain</li> <li>Upload your data</li> <li>Index the uploaded data</li> <li>Search the index</li> </ol> <p>Search requests are HTTP GETs. Pre-processed to do the following:</p> <ol> <li>Convert to lowercase</li> <li>Split on whitespace and punctuation</li> <li>Remove stopwords (\"the\" \"is\" \"at\")</li> </ol> <p>(All of these can make searching on certain terms very difficult or impossible.)</p> <p>Can specify options for facet information or ranking weights.</p> <p>Search results returned as JSON or XML.</p> <p>Creation proicess:</p> <ol> <li>Name your domain</li> <li>Search Domain Name</li> <li>Desired instance type</li> <li>Desired replication count</li> <li>Configure index</li> <li>Analyze sample files from local machine</li> <li>Analyze sample objects from S3</li> <li>Analyze sample objects from DynamoDB</li> <li>Predefined config</li> <li>Manual config</li> <li>Review index configuration</li> <li>Setup access policies</li> </ol>"},{"location":"AWS/0400_Databases_and_Application_Services/#deployment-and-management","title":"Deployment and Management","text":""},{"location":"AWS/0400_Databases_and_Application_Services/#monitoring-aws-with-cloudwatch","title":"Monitoring AWS with CloudWatch.","text":"<p>Performance monitoring for EC2, storage volumes, load balancers, databases, etc.</p> <p>Services -&gt; Administration &amp; Security -&gt; CloudWatch</p> <p>Alerts triggered when pre-defined thresholds are crossed. Can send a message to a given SNS topic. That can, in turn, send E-mails to appropriate recipients.</p> <p>Metric categories:</p> <ul> <li>CloudSearch</li> <li>DynamoDB</li> <li>EBS</li> <li>EC2</li> <li>ELB</li> <li>ElastiCache</li> <li>RDS</li> <li>S3</li> <li>SNS</li> <li>SQS</li> </ul> <p>Some metrics are per-instance, others per-group, as appropriate for the metric.</p> <p>Graphs can show a metric over time. Time spans can include:</p> <ul> <li>1 min</li> <li>5 min</li> <li>15 min</li> <li>1 hour</li> <li>6 hours</li> <li>1 day</li> </ul> <p>Can show average, minimum, maximum, sum, or \"data samples\" (number of samples?)</p> <p>Alarms on a given instance + metric can be created with \"Create Alarm\" button on the same graph screen.</p> <p>The create alarm process is two steps:</p> <ol> <li>Select Metric (already selected if we were viewing it)</li> <li>Define Alarm</li> <li>Name:</li> <li>Description:</li> <li>Whenever <code>&lt;metric&gt;</code> is <code>&gt;=</code> <code>&lt;value&gt;</code> for <code>N</code> consecutive periods</li> <li>Actions: based on <code>state</code> send notification to <code>&lt;SNS topic&gt;</code> (can also add auto-scaling actions or EC2 actions)</li> </ol>"},{"location":"AWS/0400_Databases_and_Application_Services/#creating-a-simple-application-stack-with-opsworks","title":"Creating a Simple Application Stack with OpsWorks.","text":"<p>OpsWorks is an application management platform. Manages an application lifecycle to automate things like deployment and scaling.</p> <p>Services -&gt; Deployment &amp; Management -&gt; OpsWorks</p> <p>We can create the stack from nothing or by adding existing EC2 instances.</p> <p>\"Add your first stack\" -- create from nothing.</p> <p>Options:</p> <ul> <li>Name</li> <li>Region</li> <li>VPC</li> <li>Default subnet</li> <li>Default operating system</li> <li>Default root device</li> <li>EBS backed (persistent)</li> <li>Instance store (ephemeral)</li> <li>IAM role: assists with interdependencies betweeen AWS resources</li> <li>Default SSH key</li> <li>Default IAM instance profile</li> <li>Hostname theme</li> <li>Stack color</li> <li>Advanced</li> <li>Chef version</li> <li>Custom chef cookbooks (awslabs has a bunch of them on GitHub)</li> <li>OpsWorks Agent version</li> <li>Custom JSON: passed to Chef recipes</li> <li>Use OpsWorks security groups</li> </ul> <p>Once the stack is made we can add \"Layers\" which are sets of similar EC2 instances. Layer types include things like HAproxy, static web server, Rails App server, PHP app server, Node.js app server, Java app server, AWS Flow (Ruby), MySQL, memcached, ganglia, ECS Cluster, or \"Custom.\"</p> <p>Layers can get Chef recipes assigned. Some of them are included like <code>ssh_host_key</code>, <code>ssh_users</code>, or <code>php::configure</code>.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#elastic-beanstalk-overview","title":"Elastic Beanstalk Overview","text":"<p>Automates provisioning of resources for apps running on AWS.</p> <p>Workflow:</p> <ol> <li>Create application</li> <li>Upload version</li> <li>Launch environment</li> <li>Manage environment</li> <li>Update version: go to \"Upload version\" and repeat as needed with future versions.</li> </ol> <p>The application can use whatever language we want: .NET, python, Java, Ruby, etc. Can be deployed straight from GitHub.</p> <p>\"Manage environment\" can also mean setting up appropriate capacity, load balancing, CloudWatch.</p> <p>A BeanStalk application is a collection of Beanstalk components. For example, a particular application version (collection of code) along with an environment (a deployed collection of other code, dependencies?)</p> <p>The Environment Configuration are the parameters for deployed code that define the behavior of the underlying AWS resources.</p> <p>The Configuration Template starts creating the environment.</p> <p>An application version is the code itself sitting in S3, ready to be used.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#creating-an-audit-trail-with-cloudtrail","title":"Creating an Audit Trail with CloudTrail.","text":"<p>Logs API calls realted to resources. Could be normal internal API calls or could be external API calls due to development activities, etc. These may need to be logged and saved due to regulatory requirements also.</p> <p>CloudTrail logs go into an S3 bucket.</p> <p>Services -&gt; Administraton &amp; Security -&gt; CloudTrail</p> <p>Config options:</p> <ul> <li>Create new S3 bucket (will auto-set permissions on S3 to allow logs)</li> <li>S3 bucket</li> <li>Advanced</li> <li>log file prefix</li> <li>SNS notify for every log delivery</li> <li>SNS topic</li> <li>CloudWatch Logs</li> </ul> <p>Can filter log display from the view history screen (web GUI) based on time range, resource name, etc.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#creating-a-data-pipeline","title":"Creating a Data Pipeline.","text":"<p>Copies data between data stores like:</p> <ul> <li>S3</li> <li>DynamoDB</li> <li>MySQL</li> <li>Redshift</li> </ul> <p>Scheduled based on preconditions and run when they are met.</p> <p>Can be fed to Elastic MapReduce clusters for auto-scaling.</p> <p>IAM policies control what the pipelines can do.</p> <p>AWSDataPipelineRole policy.</p> <p>Services -&gt; Analytics -&gt; Data Pipeline -&gt; Create new pipeline</p> <ul> <li>Name</li> <li>Description (optional)</li> <li>Source</li> <li>Build using template (lots of options, including)<ul> <li>Run AWS CLI command</li> <li>Export DynamoDB to S3</li> <li>Full copy RDS MySQL table to S3</li> <li>Incremental copy RDS MySQL table to S3</li> </ul> </li> <li>Import a definition</li> <li>Build using Architect</li> <li>Schedule</li> <li>Run once on pipeline activation OR on a schedule</li> <li>Run every <code>N</code> days</li> <li>Starting on pipeline activation OR given date</li> </ul> <p>The parameters are appropriate for the Source type. (e.g. login and source/target details.)</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#cloudformation-overview","title":"CloudFormation Overview.","text":"<p>Method of provisioning resources from a template via automation.</p> <p>Can automate provisioning of EC3, S3, ELB, etc.</p> <p>Services -&gt; Deployment and Management -&gt; CloudFormation</p> <p>Create New Stack or Create a template from your existing resources (CloudFormer)</p> <p>Create new stack:</p> <ul> <li>Stack</li> <li>Name</li> <li>Template</li> <li>Source<ul> <li>Select a sample template (lots of options including):</li> <li>LAMP stack</li> <li>Wordpress blog</li> <li>CloudFormer</li> <li>Windows Active Directory</li> <li>Upload a template to Amazon S3</li> <li>Specify an Amazon S3 template URL</li> </ul> </li> <li>Parameters:</li> <li>Features: Specific to your source type</li> <li>Instance Type</li> <li>KeyName (keypair)</li> <li>Roles</li> </ul> <p>Can also automate the process with Chef or Puppet.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#practice-database-monitoring","title":"Practice: Database Monitoring","text":""},{"location":"AWS/0400_Databases_and_Application_Services/#launch-a-microsoft-sql-server-db-instance","title":"Launch a Microsoft SQL Server DB Instance.","text":"<p>Services -&gt; Database -&gt; RDS -&gt; Create Database</p> <p>Microsoft SQL Server</p> <p>SQL Server Express Edition (keep it cheap!)</p> <p>Template: Free tier</p> <p>Name: <code>database-sql-server</code></p> <p>Master username: <code>admin</code> (default)</p> <p>DB instance: <code>db.t2.micro</code> (default)</p> <p>Storage type: <code>General Purpose</code> (default)</p> <p>Allocated storage: <code>20GB</code> (default)</p> <p>Connectivity:</p> <ul> <li>Virtual Private Cloud: <code>Default VPC</code> (default)</li> <li>Subnet group: <code>default</code> (default)</li> <li>Publicly accessible: <code>No</code> (default)</li> <li>VPC security group: <code>create new</code></li> <li>New VPC security group name: <code>database-sql-sg</code></li> <li>Availability zone: <code>No preference</code> (default)</li> <li>Database port: <code>1433</code> (default)</li> </ul> <p>SQL Server Windows Auth: None (default)</p> <p>There are no free AWS-hosted databases. Skip the practice and save it for a future lab.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#connect-to-the-db-instance","title":"Connect to the DB Instance","text":"<p>Skipping to avoid costs.</p>"},{"location":"AWS/0400_Databases_and_Application_Services/#monitor-the-db-instance","title":"Monitor the DB Instance","text":"<p>Skipping to avoid costs.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/","title":"Networking and Best Practice","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#route-53","title":"Route 53","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#route-53-overview","title":"Route 53 Overview.","text":"<p>Amazon's DNS service.</p> <p>Elastic: easily changed with new records added/removed</p> <p>Fault tolerant: can handle the loss of availability zones and still function</p> <p>Geo redundant: available in multiple regions</p> <p>Offers three services:</p> <ol> <li>DNS name resolution</li> <li>DNS domain registration (or migration)</li> <li>Health checking</li> </ol> <p>Global network of authoritative DNS servers (those responsible for a whole domain.)</p> <p>Route 53 can route traffic to the nearest edge location based on the client location (lowest latency.)</p> <p>Can transfer other domains. Includes support for newer domains (e.g. <code>.technology</code>)</p> <p>For health checks, route 53 can check out app to ensure it's reachable/available as well as functional.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#registering-a-domain-through-route-53","title":"Registering a Domain through Route 53.","text":"<p>Services -&gt; Networking -&gt; Route 53 -&gt; Registered Domains -&gt; Register Domain</p> <p>\"Pending Requests\" shows registration in progress</p> <p>Eventually \"Hosted Zones\" will show our registered domain.</p> <p>An <code>SOA</code> and <code>NS</code> record are created by default.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#configuring-route-53-as-your-dns-provider","title":"Configuring Route 53 as your DNS Provider.","text":"<p>Convert Amazon public DNS names to something friendlier.</p> <p>One approach: create 'A' record for the public IP of an EC2 instance.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#virtual-private-cloud","title":"Virtual Private Cloud","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#create-virtual-private-cloud","title":"Create Virtual Private Cloud.","text":"<p>Used for network isolation, similar to VLANs.</p> <p>Services -&gt; VPC -&gt; Create VPC</p> <p>Configuration:</p> <ul> <li>Name tag</li> <li>CIDR block (up to /16)</li> <li>Tenancy (default or dedicated)</li> </ul> <p>Once created we can see the VPC's DHCP options set, Route table, and Network ACL in the Summary tab.</p> <p>DHCP Options Set parameters:</p> <ul> <li>Name tag</li> <li>Domain name</li> <li>Domain name servers</li> <li>NTP servers</li> <li>NetBIOS name servers</li> <li>NetBIOS node type</li> </ul> <p>Network ACL allows both allow and deny rules. Similar config to security groups.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#configure-virtual-private-cloud-subnets","title":"Configure Virtual Private Cloud Subnets.","text":"<p>Can create subnets to separate resources by IP address range.</p> <p>Services -&gt; VPC -&gt; Subnets -&gt; Create Subnet</p> <p>Configuration items:</p> <ul> <li>Name tag</li> <li>VPC</li> <li>Availability zone</li> <li>CIDR block</li> </ul> <p>Subnets inherit the route table and network ACL from parent VPC.</p> <p>To allow subnet to auto-assign public IPs use Subnet Actions -&gt; Modify Auto-Assign Public IP. Still need an Internet Gateway configured on the VPC to allow Internet traffic in.</p> <p>Assign an Internet Gateway to the VPC using VPC -&gt; Internet Gateways.</p> <p>The VPC route table will show a destination of 0.0.0.0/0 via the newly added Internet Gateway.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#launch-an-ec2-instance-into-a-virtual-private-cloud","title":"Launch an EC2 Instance into a Virtual Private Cloud.","text":"<p>Services -&gt; EC2 -&gt; Create</p> <p>Step 3, \"Configure Instance\" has a \"Network\" field for choosing the VPC.</p> <p>New Windows instance can't seem to get to the Internet. DHCP options set a custom DNS earlier. Instead we should use the Amazon provided DNS unless there's a compelling reason not to.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#configuring-vpc-network-address-translation","title":"Configuring VPC Network Address Translation.","text":"<p>Allows VPCs to access the Internet without using public IPs.</p> <p>VPC creation process has four initial configurations:</p> <ol> <li>VPC with a Single Public Subnet</li> <li>VPC with Public and Private Subnets</li> <li>VPC with Public and Private Subnets and Hardware VPN access</li> <li>VPC with Private Subnet Only and Hardware VPN access</li> </ol> <p>To demo NAT, use option 2. Their mini-diagram shows NAT between the public and private subnets, but wouldn't the NAT really be between the public subnet and the Internet? The public subnet still uses private IPs internally, right?</p> <p>Config items for basic architecture 2:</p> <ul> <li>IP CIDR Block</li> <li>VPC Name</li> <li>Public subnet</li> <li>Availability zone</li> <li>Public subnet name</li> <li>Private subnet</li> <li>Availability zone</li> <li>Public subnet name</li> <li>NAT instance</li> <li>Instance type</li> <li>Key pair name</li> <li>S3 endpoint subnet</li> <li>Enable DNS hostnames</li> <li>Hardware tenancy</li> </ul> <p>New EC2 instance created based on an Amazon vpc-nat AMI.</p> <p>Sample EC2 instance created in the private subnet can't be reached from the Internet but can reach the Internet via NAT.</p> <p>SSH (PuTTY) can be used to reach NAT. And he finally shows us \"pageant\" to manage keys. Yay! If we can't reach the NAT instance, check security group.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#configuring-vpc-access-control-lists","title":"Configuring VPC Access Control Lists.","text":"<p>Apply to all traffic entering/leaving subnets within the VPC. Security groups can work similarly, but for specific instances.</p> <p>The Network ACL associated with a VPC is in the VPC summary when the VPC is selected from the VPC list.</p> <p>Example: set ACL to deny ICMP outbound traffic</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#direct-connect","title":"Direct Connect","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#direct-connect-overview","title":"Direct Connect Overview.","text":"<p>Allows direct physical connections to private networks on AWS.</p> <p>Amazon hardware on premises with amazon-provided network link. Traffic bypasses Internet entirely.</p> <p>1 or 10 Gigabits available.</p> <p>Great for large volume transfers, real-time data, and added data security.</p> <p>Typically the Amazon endpoint is in a DMZ to control reverse access from AWS. Sometimes, depending on the physical location, the actual connection is to an Amazon partner who in turn connects directly to AWS.</p> <p>Direct connect needs at least one virtual interface per VPC within AWS that will receive Direct Connect traffic. Additonal virtual interfaces might be needed for public AWS resources.</p> <p>Direct connect operates at Layer 2. The AWS routing must be configured into the internal on-premises routers.</p> <p>Cross Connect can allow teaming of multiple Direct Connect links.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#creating-and-deleting-a-connection","title":"Creating and Deleting a Connection.","text":"<p>Services -&gt; Networking -&gt; Direct Connect -&gt; Get Started with Direct Connect</p> <p>us-west-2 Locations include (from live AWS, not video):</p> <ul> <li>EdgeConnex Hillsboro OR</li> <li>Pittock Block, Portland OR</li> </ul> <p>Since this is a physical install, we would provide contact info and AWS will send a \"letter of authorization\" to the physical address asking for specific information related to the physical install. For example, we would provide info about our on-premises router. (New method appears to be a download of the LOA which we would take to an APN partner like the two above.)</p> <p>Provision virtual interfaces for public AWS resources or our private VPCs.</p> <p>To remove direct connect:</p> <ol> <li>Delete virtual interfaces</li> <li>Delete Direct Connect config</li> <li>Cancel cross connect network services</li> </ol>"},{"location":"AWS/0500_Networking_and_Best_Practice/#working-with-virtual-interfaces","title":"Working with Virtual Interfaces.","text":"<p>Need to have a Direct Connect connection configured to create Virtual Interfaces.</p> <p>Public virtual interfaces allow us to use public AWS resources (e.g. S3, Glacier) using their public IPs but routed over that Direct Connect connection.</p> <p>Private virtual interfaces let us reach a VPC using private IP addresses. One virtual interface per VPC.</p> <p>Virtual interfaces (layer 2) use 802.1Q VLAN tags and can apply to the whole AWS cloud or a single specific VPC.</p> <p>Routing handled via Border Gateway Protocol (BGP.)</p> <p>The AWS console can provide router-specific configuration details needed to make the virtual interface work with our on-premises router.</p> <p>Helpful to have at least one EC2 instance per VPC running so that simple connectivity testing can be done.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#cloud-best-practice","title":"Cloud Best Practice","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#design-for-failure","title":"Design for Failure.","text":"<p>Assume things will fail (a common assumption for high availability.)</p> <p>Design for automated recovery. For example, Auto Scaling groups can terminate misbehaving instances.</p> <p>Where are the single points of failure? For example:</p> <ul> <li>Load balancers</li> <li>Cluster master nodes</li> </ul> <p>What happens when one of those single points of failure dies? How do we know when it has failed? (What about poor performance vs. hard failure?) How will the failed node be replaced?</p> <p>Auto Scaling groups is a common way to auto-create new nodes when others fail.</p> <p>How does a failover happen? How will users be impacted during a failover event?</p> <p>How does the application handle lack of response from a dependent service? (e.g. app server no longer getting responses from its back-end database.) Applications need to have built-in exception handling.</p> <p>How are changes in dependent service APIs handled?</p> <p>Cloud architecures should handle rebooting or relaunching (new HW) an instance without the whole service going offline.</p> <p>For SQS and SimpleDB plan around having a controller thread restart on failure. Ensure apps handle the controller restart appropriately.</p> <p>Use Elastic IPs for public access-- move IP from a dead instance to a functional one.</p> <p>Keep AMIs with customization on hand to quickly restore an environment from the AMI.</p> <p>Use multiple Availability Zones to guard against AZ failure.</p> <p>EBS and automated snapshots can allow for a limited roll-back ability, but additional backups of some sort are also needed.</p> <p>CloudWatch should be used to monitor resource capacity.</p> <p>RDS automated backups should be used to allow for recovery of accidentally removed data.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#decouple-components","title":"Decouple Components.","text":"<p>Loosely coupled components scale better.</p> <p>Build components (apps) that tolerate other components which:</p> <ul> <li>Fail</li> <li>Pause</li> <li>Respond slowly</li> </ul> <p>Components are treated as \"black boxes\" with the internals hidden. They can be re-used in different contexts by following the published access methods.</p> <p>Use asynchronous communications rather than waiting for a (possibly slow) reply.</p> <p>No dependencies across application layers.</p> <p>One example of loose coupling-- replace direct API calls with message queues.</p> <p>Add components to an AMI so they can be launched and immediately used. (Me: automate the creation of AMIs from known good base components so we can easily make repeatable changes to the underlying AMI.)</p> <p>Design components with \"service interfaces\" where things like a timeout can be added to reduce tight coupling.</p> <p>Make applications stateless so if the back-end app dies, its state loss doesn't matter.</p> <p>Use SQS to isolate apps from one another and buffer their communications, reducing tight coupling.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#implement-elasticity","title":"Implement Elasticity.","text":"<p>Allows rapid add/remove of AWS resources.</p> <p>Auto Scaling can be based on demand or proactively scheduled. (e.g. seasonal variation, in response to marketing campaign, etc.)</p> <p>Automation is a big advantage that cloud has built in. Making deployments automated is critical to getting the best advantage from the cloud. Automation reduces errors and inconsistencies and enables easier scaling.</p> <p>Chef recipes can help with automating the configuration of the EC2 instance contents themselves. (e.g. users, packages, third party software, etc.)</p> <p>Deploy with custom AMIs which could contain specific instance configurations.</p> <p>Bootstrapping sets up the Chef client within the OS of the instance so that further customization can be done more easily. The Chef client will pull any client-image-specific changes and customizations that are needed and apply them.</p> <p>Bootstrapping also allows instances to be attached to clusters. (What kind of clusters? Unclear...)</p> <p>Can auto-provision more storage when needed (is this the only way?)</p> <p>Environments like development, staging, and production are easily re-created and deployment errors are reduced when using Bootstrapping.</p> <p>App components should be location-agnostic (part of making them loosely coupled.)</p> <p>Consider using Just Enough Operating System (JeOS), a minimally configured host OS. Similar to virtual appliances. This can be managed using AMIs.</p> <p>Use Auto Scaling Groups.</p> <p>Learn and use Chef, Puppet, etc. They can cost some time to learn, but save time in re-used code and increased consistency later on.</p> <p>EBS volumes boot faster than instance stores.</p> <p>Use SimpleDB to store and retrieve configuration data.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#parallelize","title":"Parallelize.","text":"<p>It's easy to run multiple processes in cloud environments. It's easy to add more instances so the ability to run many tasks in parallel on freshly created instances allows virtually unlimited scaling.</p> <p>Multiple threads should be used, all services should be thread-safe and use a share-nothing architecture. Each component should be able to run independently of other components.</p> <p>Distribute incoming web connections using a load balancer. Web servers should process asyncronously-- each web server process should be independent from the others.</p> <p>Batch processing should use multiple processing units (slave units) which makes it possible to use Elastic MapReduce to scale out our processing.</p> <p>Use multi-threaded S3 requests and multi-thread SimpleDB <code>GET</code> and <code>BATCHPUT</code>.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#data-location","title":"Data Location.","text":"<p>Dynamic data should be at a location with low latency compared to its processing location.</p> <p>Static data should be kept at a locataion close to the end user. Static data includes things like PDFs, CSS, and JavaScript. CloudFront can help cache this info close to end users for a faster web experience.</p> <p>If data is generated in the cloud, ideally any consumers of that data also run in the cloud (in the same region, though keep redundancy in mind.)</p> <p>For large imports, consider physical shipping. </p> <p>Launch cluster members within the same availability zone.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#security-best-practice","title":"Security Best Practice","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#aws-shared-responsibility-model","title":"AWS Shared Responsibility Model.","text":"<p>Amazon responsible for:</p> <ul> <li>Facilities</li> <li>Hardware physical security</li> <li>Network infrastructure</li> <li>Virtualization infrastructure (e.g. hypervisors)</li> </ul> <p>Customer responsible for:</p> <ul> <li>Amazon Machine Images and OS</li> <li>Applications</li> <li>Configuration and policies applied to AWS resources</li> <li>Credentials (IAM + other)</li> <li>Data at rest</li> <li>Data in transit</li> <li>Data stores</li> </ul> <p>Container services model is different as Amazon manages more of the underlying infrastructure, up through the OS hosting the containers.</p> <p>In the Abstracted Services model, Amazon manages even more, up to the client-side data being managed by the customer.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#define-and-categorize-assets","title":"Define and Categorize Assets.","text":"<p>When designing an Information Security Management System (ISMS) we should work out a way where the effort of securing assets is appropriate for the level of protection they need.</p> <p>Qualitative categorization uses expected risks and their expected likelihood to help prioritize our efforts.</p> <p>Quantitative categorization uses financial impacts to help prioritize our efforts.</p> <p>Once threats are identified, solutions can be made to protect against the threats by mitigation or prevention.</p> <p>Essential assets:</p> <ul> <li>Processes (business processess like cloud computing)</li> <li>Activities (for achieving business goals)</li> <li>Business information (off-site backups to protect from fire, for example)</li> <li>Reputation</li> </ul> <p>Support assets:</p> <ul> <li>Personnel</li> <li>Hardware</li> <li>Sites</li> <li>Partners</li> </ul> <p>Build an asset matrix with info like:</p> <ul> <li>Owner</li> <li>Category (sensitivity level)</li> <li>Dependencies (network, database)</li> <li>Asset sensitivity (dupe?)</li> <li>Related costs (deployment, maintenance, replacement, etc.)</li> </ul>"},{"location":"AWS/0500_Networking_and_Best_Practice/#design-isms-to-protect-aws-assets","title":"Design ISMS to Protect AWS Assets.","text":"<p>Start with business objectives. For example, storing trade secrets in an S3 bucket will require careful handling to ensure only trusted people have access to the underlying information. (Me: use client-side decryption with a good key management and auditing system.)</p> <p>Certain regulations might need to be followed, such as a requirement to retain certain records for a given length of time.</p> <p>The size of the organization and its internal business processes might determine which methods are easiest to use.</p> <p>An Information Security Management System is a phased approach to apply security. A standard framework like ISO 27001 can help.</p> <p>Steps in building ISMS:</p> <ol> <li>Define scope (AWS services which need to be secured)</li> <li>Create a policy (what data needs to be encrypted, for example)</li> <li>Risk assessment (evaluate threats and apply controls)</li> <li>Choose a framework (ISO 27001, ISO 12207)</li> <li>Get management approval</li> <li>Create a statement of applicability (describes the specific controls that are to be used)</li> </ol>"},{"location":"AWS/0500_Networking_and_Best_Practice/#manage-accounts","title":"Manage Accounts.","text":"<p>Ensure just enough permissions granted (principle of least privilege)</p> <p>IAM users + groups makes assigning permissions easier.</p> <p>Special account: the AWS account</p> <ul> <li>Used to sign up and represents the business relationship with AWS</li> <li>Has root permission to all resources</li> <li>Multiple AWS accounts can exist (can multiple root accounts per AWS account exist?)</li> </ul> <p>Normal accounts: IAM accounts</p> <ul> <li>Multiple IAM users under a single AWS account</li> <li>Can be a person or application</li> <li>Fine grained permissions</li> </ul> <p>Scenarios for multiple AWS accounts</p> <ul> <li>Prod, dev, and test might be under different accounts</li> <li>Multiple departments with autonomoy</li> <li>Centralized security with multiple autonomous projects</li> <li>One AWS account for centralized common resources</li> <li>Separate AWS accounts per autonomous project</li> </ul> <p>Credential types:</p> <ul> <li>Sign-in: like username/password + MFA</li> <li>Programmatic: access keys or developer-created MFA-protected API calls</li> </ul> <p>Delegation needed when:</p> <ul> <li>Applications on EC2 require AWS resource access</li> <li>Access between AWS accounts</li> <li>Corporate identity federation</li> </ul>"},{"location":"AWS/0500_Networking_and_Best_Practice/#manage-access-to-ec2","title":"Manage Access to EC2.","text":"<p>OS-level access to EC2 instances uses OS-level credentials, not AWS-level credentials.</p> <p>Customers own and change the OS credentials, but AWS can help bootstratp this process via EC2 key pairs. Other options include X.509 certificates, Microsoft AD, or OS-local authentication.</p> <p>AWS provides asymmetric RSA key pairs. (public/private key) and can be moved between IAM or AWS accounts via import.</p> <p>Linux uses the <code>cloud-init</code> daemon to grant initial access. This appends the key pair defined during instance creation to the <code>authorized_keys</code> file.</p> <p>(Connection demo using PuTTY... isn't this a review?)</p> <p>Windows <code>ec2config</code> service sets a random admin password, and encrypts it with the public key used during EC2 instance creation.</p> <p>We then need to submit our private key via web form (WHICH IS A TERRIBLE IDEA FOR SECRET INFORMATION) to decrypt the admin password. This is a terrible idea because:</p> <ul> <li>It trains end users that submitting secret keys to websites is normal and accepted</li> <li>It could allow the secret key to be intercepted by a man-in-the-middle attack</li> <li>We can no longer tell auditors that a secret key has never left our control</li> </ul> <p>We should be sure that Windows key pairs are never used for any purpose beyond intial Windows instance access.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#s3-data-at-rest","title":"S3 Data at Rest.","text":"<p>Data at rest concerns:</p> <ul> <li>Deletion</li> <li>Disclosure</li> <li>Modification</li> <li>Availabilty</li> </ul> <p>Methods to help mitigate these issues</p> <ul> <li>Permissions (controls who can do the above activities)</li> <li>Bucket-level</li> <li>Object-level</li> <li>IAM policies</li> <li>Versioning (allows an undo if an activity was unwanted)</li> <li>Disabled by default</li> <li>Protects from deletion or modification at increased risk of disclosure</li> </ul> <p>Replication improves availability of S3 data by replicating it across all the availability zones in a region. Won't protect from deletion or modification.</p> <p>Backups can also protect from deletion or modification. S3 backups are not automated, though we can configure a backup to Glacier. OS-level backups may still be needed for data within instances. (EBS?)</p> <p>S3 server-side encryption can be done easily and transparently to the end user. Each object is encrypted with a unique AES256 key. That key is encrypted with a master AES256 key which is rotated and used for auto-decryption.</p> <p>S3 client-side encryption can also be used, but requires us to manage our own keys outside of AWS. The data is encrypted with our key on the way to S3 and the key itself is kept by us outside of AWS. The application reading the S3 data will decrypt it. (Hopefully there are well-audited libraries to handle this for developers since getting encryption done right is very hard.)</p> <p>There's an AWS Encryption SDK to use for this though it might be best to avoid using an SDK published by the same company holding the data since if Amazon itself were compromised, that SDK might itself have leaks found or introduced.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#ebs-data-at-rest","title":"EBS Data at Rest.","text":"<p>EBS volumes are stored internal to AWS as files. Two copies are kept in the same availability zone (mirroring) which allows the data to survive an underlying hardware failure but not a AZ-wide disaster.</p> <p>EBS snapshots can be created as a backup with independent permissions. An encrypted EBS volume always creates encrypted EBS snapshots.</p> <p>Windows can use Encrypting File System (EFS) for local-to-the-instance encryption of the OS data. Bitlocker can also be used but only works with a password since EC2 doesn't include any Trusted Platform Module support.</p> <p>Linux can use <code>dm-crypt</code> for transparent encryption and key management.</p> <p>Other third party tools which can be used:</p> <ul> <li>TrueCrypt (discontinued in 2014. What year was this video made?)</li> <li>SafeNet ProtectV</li> </ul>"},{"location":"AWS/0500_Networking_and_Best_Practice/#rds-data-at-rest","title":"RDS Data at Rest.","text":"<p>AES256 used to encrypt RDS data. This must be specified when the database is created.</p> <p>There may be additional crypto APIs available from specific database instances.</p> <p>Apps can use PKI certificates to help encrypt/decrypt database info.</p> <p>Instance-Specific APIs:</p> <ul> <li>MS SQL</li> <li>Transact-SQL functions for encryption, signing, and hashing</li> <li>MySQL</li> <li>Oracle</li> <li>Oracle Transparent Data Encryption (Enterprise Edition)</li> <li>Bring your own license only for EE</li> </ul> <p>(no PostgreSQL?)</p> <p>For storing personally identifiable info (PII) it may be better to obfuscate the info using a one-way hash.</p> <p>RDS encryption not available for Amazon Aurora.</p> <p>Selective encryption of database fields can create problems for data lookups since the encrypted data may not be found.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#practice-aws-best-practices","title":"Practice: AWS Best Practices.","text":"<p>Goal: Build a 3 tier web application on AWS</p> <p>Questions:</p> <ul> <li>How do you ensure high availability?</li> <li>How do you enable scalability</li> <li>What are the data-at-rest security considerations at each tier?</li> </ul>"},{"location":"AWS/0500_Networking_and_Best_Practice/#high-availabilityscalability","title":"High availability/Scalability","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#web-front-end","title":"web front end","text":"<p>Use route 53 health checks across at least two regions to provide DR if an AZ fails as well as reducing end user latency on web requests.</p> <p>Use SQS for fetching app server info.</p> <p>An Amazon AMI is built using automated/scripted methods to serve as the web front end immediately upon deployment.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#application-server","title":"application server","text":"<p>Use automatic scaling groups with a minimum of two instances split across AZs.</p> <p>memcached will be used to cache database info locally for repeated application server queries to the same records.</p> <p>Use SQS for fetching database data and for passing info to the web server.</p> <p>An Amazon AMI is built using automated/scripted methods to serve as the application server immediately upon deployment.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#database","title":"database","text":"<p>Consider either a multi-AZ deployment or the Amazon Aurora Global Database if multi-region redundancy is desired.</p> <p>Use SQS for passing info to the app server.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#data-at-rest-security","title":"Data at Rest Security.","text":"<p>Like most web applications, availability and data integrity are more important than unauthorized disclosure so there is no use of encryption.</p> <p>Static web objects will use S3 versioned storage for simplified recovery in the event of unwanted modification.</p> <p>The Amazon AMIs mitigate the need for regular backups of the web or app server instances.</p> <p>The database will have snapshots taken regularly.</p> <p>The database will go into a separate VPC so that access can be more easily controlled.</p>"},{"location":"AWS/0500_Networking_and_Best_Practice/#provided-solution","title":"Provided Solution","text":""},{"location":"AWS/0500_Networking_and_Best_Practice/#high-availability","title":"high availability","text":"<ul> <li>Design for failure</li> <li>Identify single points of failure</li> <li>Graceful failover with elastic IPs</li> <li>Maintain AMIs to restore environments</li> <li>Use Availability Zones</li> <li>Use EBS and automated snapshots</li> <li>Use CloudWatch for monitoring</li> <li>Use RDS automated backups</li> </ul>"},{"location":"AWS/0500_Networking_and_Best_Practice/#scalability","title":"scalability","text":"<ul> <li>Decouple components</li> <li>Bundle components with AMIs</li> <li>Design components with service interfaces</li> <li>Make applications stateless</li> <li>Use SQS message queueing</li> <li>Use auto scaling groups</li> <li>Implement elasticity</li> <li>App components location-agnostic</li> <li>Reduce OS footprint in AMI (Just Enough Operating System)</li> <li>Use auto scaling groups</li> <li>Use deployment / configuration management tools (Chef / Puppet / Ansible / Terraform)</li> <li>Monitor system metrics and set auto-scaling options</li> <li>Store config information using SimpleDB (off-server)</li> </ul>"},{"location":"AWS/0500_Networking_and_Best_Practice/#security","title":"security","text":"<ul> <li>database</li> <li>Use internal crypto functions</li> <li>Hash PII</li> <li>Put the DB in separate secured VPC (good idea!)</li> <li>app servers</li> <li>encryption local to OS</li> <li>web servers</li> <li>S3 server-side encryption</li> </ul>"},{"location":"AWS/2000_Well_Architected_Framework/","title":"AWS Well-Architected Framework (WAF)","text":""},{"location":"AWS/2000_Well_Architected_Framework/#introduction","title":"Introduction","text":"<p>The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#aws-well-architected-framework-pillars","title":"AWS Well-Architected Framework Pillars","text":""},{"location":"AWS/2000_Well_Architected_Framework/#operational-excellence-pillar","title":"Operational Excellence Pillar","text":"<p>Keep things running and react appropriately to unusual events.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#security-pillar","title":"Security Pillar","text":"<p>Protect the confidentiality, integrity, and availability of the information kept in AWS.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#reliability-pillar","title":"Reliability Pillar","text":"<p>Similar to the \"availability\" portion of security, but this is about being resistant to unexpected events rather than deliberate attacks.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#performance-efficiency-pillar","title":"Performance Efficiency Pillar","text":"<p>Use resources with as little waste as practical.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#cost-optimization-pillar","title":"Cost Optimization Pillar","text":"<p>Find the lowest cost solution which meets the business needs.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#definitions","title":"Definitions","text":""},{"location":"AWS/2000_Well_Architected_Framework/#component","title":"component","text":"<p>A collection of AWS resources that work together to fulfil a particular need. Often used as a collection of tightly coupled items within a much larger collection of loosely coupled components.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#workload","title":"workload","text":"<p>A loosely coupled group of components that fulfil some buiness role.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#milestones","title":"milestones","text":"<p>Beware the waterfall model! These can mark more significant changes made to the architecture, but if we get continuous delivery going these should either be extremely common or absent.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#architecture","title":"architecture","text":"<p>How all the components work together and interact to produce the desired workload.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#technology-portfolio","title":"technology portfolio","text":"<p>A grouping of multiple workloads.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#trade-offs","title":"Trade-offs","text":""},{"location":"AWS/2000_Well_Architected_Framework/#fast","title":"Fast","text":"<p>This usually refers to the speed of development and implementation. More planning and/or more iterations let optimizations for performance and cost happen so when more time is taken the architecture can improve.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#cheap","title":"Cheap","text":"<p>Often components with lower reliability are cheaper. Costs can also be lower if we can accept a poor response time by overcommitting resources.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#good","title":"Good","text":"<p>Often this means \"reliable\" or \"not too expensive to operate\" but can also reflect on the ultimate end-user experience. As usual, if we spend more time and money on a highly redundant, well-scaled workload it will be higher quality.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#architecture-group","title":"Architecture group","text":"<p>\"At AWS, we prefer to distribute capabilities into teams rather than having a centralized team with design capability.\"</p>"},{"location":"AWS/2000_Well_Architected_Framework/#general-design-principles","title":"General design principles","text":""},{"location":"AWS/2000_Well_Architected_Framework/#stop-guessing-your-capacity-needs","title":"Stop guessing your capacity needs","text":"<p>I'm impressed that this is the first one listed since \"premature optimization\" is such a common issue, and has been for decades. Rather than guess about capacity needs, measure them. Start small and scale up as needed.</p> <p>In order to determine \"need\" we will need good measurements of actual end-user experiences (and not just system level metrics, which are at best a loose proxy for the end user experience. At worst, they're completely decoupled from that experience.)</p>"},{"location":"AWS/2000_Well_Architected_Framework/#test-systems-at-production-scale","title":"Test systems at production scale","text":"<p>The flexibility of AWS makes this possible. With apropriate automation it becomes not only possible, but trivial.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#automate-to-make-architectural-experimentation-easier","title":"Automate to make architectural experimentation easier","text":"<p>What was I saying about automation? This enables safety features like \"revert back\" as well as reducing small errors that appear when changes are made by hand.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#allow-for-evolutionary-architectures","title":"Allow for evolutionary architectures","text":"<p>Basically, the above automation can allow for more, smaller changes vs. fewer, larger changes.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#drive-architectures-using-data","title":"Drive architectures using data","text":"<p>Use science to measure and adjust instead of taking a blind guess.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#improve-through-game-days","title":"Improve through game days","text":"<p>This might be something good to combine with Value Stream Mapping to see how long it takes us to react to and complete common tasks.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#aws-operational-excellence","title":"AWS Operational Excellence","text":""},{"location":"AWS/2000_Well_Architected_Framework/#perform-operations-as-code","title":"Perform operations as code","text":"<p>AMEN, BROTHER AWS! This is completely appropriate as the first thing listed as this can enable so many other good behaviors.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#annotate-documentation","title":"Annotate documentation","text":"<p>This seems to be a nod to the difficulties with keeping documentation up-to-date. I agree that documentation should be something that the end-users can easily update. I'm not sure I completely agree with the specifics of \"automatically annotate documentation\", but it's also possible that I misunderstand what they mean. This will be something to keep an eye on as I learn more.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#make-frequent-small-reversible-changes","title":"Make frequent, small, reversible changes","text":"<p>Agreed! It's much easier to undo a small change than a large one. Those tend to be effectively irreversible, which substantially increases the risk of outages or other unwanted behaviors that require a large, immediate effort to mitigate.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#refine-operations-procedures-frequently","title":"Refine operations procedures frequently","text":"<p>Yes! The feedback loop from operations back to the creators of an architecture is one of the most neglected yet most important. If the \"documented procedures\" aren't being followed, find out what is being used instead rather than attempting to force compliance.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#anticipate-failure","title":"Anticipate failure","text":"<p>Most of the time there are enough real failures that creating false ones is overkill. Perhaps someday we'll get things so stable that we end up needing something like the Netflix Chaos Monkey.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#learn-from-all-operational-failures","title":"Learn from all operational failures","text":"<p>Avoid blame and encourage transparency. Assume the best intentions of workers.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#best-practices-for-operational-excellence-prepare-operate-evolve","title":"Best practices for Operational Excellence: Prepare, Operate, Evolve","text":""},{"location":"AWS/2000_Well_Architected_Framework/#prepare","title":"Prepare","text":"<p>This can involve using common, well-documented standards that are prepared in advance. (Ideally implemented from coded descriptions rather than human action.) However, they also caution \"Implement the minimum number of architecture standards for your workloads.\" in order to ensure that the few standards that are present will be followed.</p> <p>Workloads are designed with methods to measure and monitor the behavior of the business applications and their effects on the infrastructure.</p> <p>The end-user (customer) experience is also measured and included in ongoing evaluation.</p> <p>Verify that changes are ready for production through the use of (automated!) checklists against the defined standards. This includes documentation checks. (<code>awspec</code>?)</p> <p>Test failures or other issues in the pre-prod testing area. (That might be a good place to let the Chaos Monkey go nuts even if we're not ready for prod testing to that degree.)</p> <p>CloudFormation as a way to set up the same workload in multiple different locations for both testing and (finally) production.</p> <p>Operational data collected using CloudTrail, VPC Flow Logs, or OS agents (<code>collectd</code>.) Analyzed/checked using CloudWatch.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#operate","title":"Operate","text":"<p>\"Define expected outcomes\" is great advice, but if the customer is unhappy and \"expected outcomes\" are being met, it may be worth a re-evaluation. This is not necessarily free of charge, but our goal is client satisfaction and that can be acccomplished by meeting expectations. When that's not happening there are pretty much two options:</p> <ol> <li>Achieve better results OR</li> <li>Adjust expectations</li> </ol> <p>\"Ensure that if an alert is raised in response to an event, there is an associated process to be executed, with a specifically identified owner.\"</p> <p>\"AWS provides workload insights through logging capabilities including Amazon Web Services AWS Well-Architected Framework AWS X-Ray, CloudWatch, CloudTrail, and VPC Flow Logs enabling the identification of workload issues in support of root cause analysis and remediation.\"</p> <p>\"Manual processes for deployments, release management, changes, and rollbacks should be avoided.\"</p> <p>\"Align metrics to business needs so that responses are effective at maintaining business continuity.\"</p>"},{"location":"AWS/2000_Well_Architected_Framework/#evolve","title":"Evolve","text":"<p>\"Dedicate work cycles to making continuous incremental improvements.\"</p> <p>This is a nice way of saying that we need to MAKE TIME for constant improvement. My general rule of thumb is a MINIMUM of 2 hours per week for some sort of strategic improvement and in almost every case 4 hours per week will give a better outcome. This means the following:</p> <ol> <li>During that time someone else must handle interruptions, this can be managed within a team by alternating times of coverage and strategic improvement between team members.</li> <li>There will be some (possibly important) short term work that does not get done due to the above longer term time investment. Pick the least impactful items, drop/defer them, and don't look back.</li> </ol> <p>\"Include feedback loops within your procedures to rapidly identify areas for improvement and capture learnings from the execution of operations.\"</p> <p>Hmmm... sounds like a counter to my earlier rant about the lack of feedback from operations back into the design.</p> <p>\"Share lessons learned across teams to share the benefits of those lessons.\"</p> <p>Yes! Writing things down is a good first step, and one that is worth doing even if nobody else reads it. (Possible case in point right here!)</p> <p>\"The AWS service that is essential to Operational Excellence is AWS CloudFormation, which you can use to create templates based on best practices.\"</p> <p>References:</p> <ul> <li>DevOps and AWS</li> <li>Operational Excellence Pillar</li> </ul>"},{"location":"AWS/2000_Well_Architected_Framework/#security","title":"Security","text":""},{"location":"AWS/2000_Well_Architected_Framework/#implement-a-strong-identity-foundation","title":"Implement a strong identity foundation","text":"<p>If we don't know who exactly people are, there's no way to know who did what and no way to keep people out of resources they don't need to be in. This includes special care with \"role\" accounts that aren't associated with a person. There should be no way for a person to ever use role account credentials and any such attempts should be quickly detected and acted upon appropriately.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#enable-traceability","title":"Enable traceability","text":"<p>In addition to alerting, having good audit logs available to assist in manual investigations is very important. Where possible automatic actions should be configured, but use caution not to sacrifice availability for confidentiality or your security responses could be used against you as an attack vector.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#apply-security-at-all-layers","title":"Apply security at all layers","text":"<p>The \"hard exterior\" approach is incompatible with modern uses of computing resources. Instead, create multiple softer boundaries.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#automate-security-best-practices","title":"Automate security best practices","text":"<p>This includes automated checks for security configurations and automatic implementations using Infrastructure as Code methods.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#protect-data-in-transit-and-at-rest","title":"Protect data in transit and at rest","text":"<p>Classify data and use encryption and tokenization as needed to keep data safe. Tokenization is especially important for HIPPA and GPDR to anonymize personal information.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#keep-people-away-from-data","title":"Keep people away from data","text":"<p>This helps with both confidentiality and integrity. By having an automated, version-controlled method for working with data a clear record of what happened when is kept, often with the ability to revert back. The only human error will be when changes are introduced into the automated systems, which provides an opportunity for automated sanity checks and/or peer reviews as appropriate. Those checks can prevent the human errors from actually reaching the data and causing permanent harm.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#prepare-for-security-events","title":"Prepare for security events","text":"<p>Running reaction exercises is a good plan given how infrequently security events actually happen. Ones that track and record the human responses are especially good since those can identify specific people who might need extra training or experience. (e.g. phishing E-mail tests.)</p>"},{"location":"AWS/2000_Well_Architected_Framework/#best-practices-for-security-identity-and-access-management-detective-controls-infrastructure-protection-data-protection-and-incident-response","title":"Best Practices for Security: Identity and Access Management, Detective Controls, Infrastructure Protection, Data Protection, and Incident Response","text":""},{"location":"AWS/2000_Well_Architected_Framework/#identity-and-access-management","title":"Identity and Access Management","text":"<p>Define principals (users, roles, groups, and services) and assemble them in an org-appropriate way. Assign policies to the appropriate principals (avoid direct assignment to users) using the principle of least-privilege.</p> <p>Programmatic access (API calls) should use temporary limited-privilege credentials like those from AWS Security Token Service..</p>"},{"location":"AWS/2000_Well_Architected_Framework/#role-vs-user","title":"Role vs. User","text":"<p>Based on the AWS Identity and Access Management User Guide a role is like a user except a role has no way to log in (no credentials) and instead is assumed by a user temporarily to perform some specific task.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#detective-controls","title":"Detective Controls","text":"<p>What is a detective control? They're methods used to find issues with processes themselves, often as a form of a \"double check\" that two different methods which are supposed to give the same result actually do. For example, checking actual inventory vs. the inventory based on recorded changes.</p> <p>Amazon GuardDuty is mentioned, but not a lot of specifics are given on examples of Detective Controls in use.</p> <p>Other supporting tools for this are AWS CloudTrail, Amazon CloudWatch, and AWS Config.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#infrastructure-protection","title":"Infrastructure Protection","text":"<p>There was some discussion of making \"hardened\" AMIs, which is great, but seems to skip over the elephant in the room of cloud infrastructure security: the permissions needed to control normal infrastructure changes are exactly the same ones that can be used to destroy it. Leaked AWS credentials frequenly lead to infrastructure destruction or misuse and an awareness of this possibility seems to be important, yet not mentioned here.</p> <p>Like the detective controls section, there is a lot in here about how we \"are able to...\" do various things, but nothing about how to do that or what the best practices might be-- a glaring omission from a best practices document.</p> <p>The supporting tools mentioned for this are Amazon VPC, Amazon CloudFront, AWS Shield (DDoS protection), and Amazon Web Application Firewall.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#data-protection","title":"Data Protection","text":"<p>They mention versioning as a way to protect data from accidental or malicious changes. I agree-- it's a good plan from a data integrity standpoint. (Less good from a data confidentiality standpoint since accidental releases of confidential info are harder to undo.)</p> <p>While they mention the importance of data classification, no specifics are given. Tags seem like they would be a really good way to make this process much easier by classifying objects appropriately. It would even be possible to automate the process of exploring the data classification of a number of interconnected objects and finding the most sensitive classification among them. That might help determine if there are unexpected relationships between sensitive and non-sensitive systems or determine if an object has the wrong overall classification. (e.g. if we attach sensitive storage to a non-sensitive VM, that VM might need to be re-classified.)</p> <p>Amazon Macie sounds a lot like what was just mentioned.</p> <p>Amazon Key Management Service makes managing keys for encryption simpler.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#incident-response","title":"Incident Response","text":"<p>The ability to spin up a forensics investigation environment was a nice observation. The cloud environment also lends itself well to quick system imaging via snapshots.</p> <p>Amazon services mentioned include IAM, CloudFormation, and CloudWatch + AWS Lambda.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#security-resources","title":"Security Resources","text":"<ul> <li>AWS Cloud Security</li> <li>AWS Compliance</li> <li>AWS Security Blog</li> <li>Security Pillar</li> <li>AWS Security Overview</li> <li>AWS Security Best Practices</li> <li>AWS Risk and Compliance</li> </ul>"},{"location":"AWS/2000_Well_Architected_Framework/#reliability","title":"Reliability","text":"<p>This relates to handling unexpected failures and unexpected performance issues.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#test-recovery-procedures","title":"Test recovery procedures","text":"<p>It's easier to simulate failures in the cloud to test recovery procedures. This can range from setting up a test environment for specific failures or testing a complete AZ failure.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#automatically-recover-from-failure","title":"Automatically recover from failure","text":"<p>Tie automated changes to the results of monitoring-- but don't forget the notification. Rather than relying on a human response to act, we act and notify and rely on humans to come back and undo it if the decision was incorrect.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#scale-horizontally","title":"Scale horizontally","text":"<p>This helps avoid single points of failure and reduces the costs of scaling. Sometimes hard to do (database) but still worth considering whenever possible.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#stop-guessing-capacity","title":"Stop guessing capacity","text":"<p>Measure and adapt instead of trying to view the future.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#manage-change-in-automation","title":"Manage change in automation","text":"<p>Rather than managing servers or other infrastructure, manage the automation that changes them. (Infrastructure as Code.)</p>"},{"location":"AWS/2000_Well_Architected_Framework/#best-practices-for-reliability-foundations-change-management-and-failure-management","title":"Best Practices for Reliability: Foundations, Change Management, and Failure Management","text":""},{"location":"AWS/2000_Well_Architected_Framework/#foundations-for-reliability","title":"Foundations for Reliability","text":"<p>This covers the basics and can include things outside the scope of AWS. For example, be sure that the network connections to AWS have sufficient bandwidth and are monitored for their health. Choose regions that are network-close to the most latency-sensitive consumers.</p> <p>Ensure the service limits are appropriate-- this includes billing limits though that is not spelled out. If Amazon can't charge you, they're going to suddenly stop your service. AWS Trusted Advisor can help show these limits. AWS Shield provides DDoS protection.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#change-management-for-reliability","title":"Change Management for Reliability","text":"<p>\"Being aware of how change affects a system allows you to plan proactively...\"</p> <p>That's true, however, in modern complex environments there's no way to predict the emergent behavior that results from the interactions of all the many parts. Thinking otherwise is to downplay the risks involved in making changes. While one answer is to cease making changes, there is another approach that cloud enables well-- quick reversion to the prior (good) state.</p> <p>Some of the normal aspects of operation that previously might have required a cumbersome approval process can instead be automated.</p> <p>AWS CloudTrail and AWS Config can help with tracking changes. Amazon Auto Scaling can help with load or performance related issues, especially when combined with CloudWatch.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#failure-management-for-reliability","title":"Failure Management for Reliability","text":"<p>\"In any system of reasonable complexity it is expected that failures will occur.\" That sounds a lot like that \"emergent behavior\" I was talking about earlier.</p> <p>\"Rather than trying to diagnose and fix a failed resource that is part of your production environment, you can replace it with a new one and carry out the analysis on the failed resource out of band.\" That's a good point, and even the analysis is somewhat optional unless it becomes a regular problem. This does require the ability to create the same server in a repeatable way. (Infrastructure as Code again.)</p> <p>Backups must be able to protect from both loss of infrastructure as well as bugs or other errors that damage data through software.</p> <p>AWS CloudFormation helps create the ability to quickly bring up additional or replacement resources. Amazon S3 is useful for backups, and Glacier is useful for archived backups.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#performance-efficiency","title":"Performance Efficiency","text":""},{"location":"AWS/2000_Well_Architected_Framework/#democratize-advanced-technologies","title":"Democratize advanced technologies","text":"<p>Sounds like Amazon marketing their expensive services... transcoding, machine learning, etc.</p> <p>\"We built all this for Netflix anyhow, maybe you can use it too?\"</p>"},{"location":"AWS/2000_Well_Architected_Framework/#go-global-in-minutes","title":"Go global in minutes","text":"<p>This is indeed one of the nice things about cloud-- get a global presence without having to build and maintain a data center in multiple spots.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#use-serverless-architectures","title":"Use serverless architectures","text":"<p>\"storage services can act as static websites\" things like this are good to keep in mind. The trick is either building or migrating traditional server-based applications to make use of it.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#experiment-more-often","title":"Experiment more often","text":""},{"location":"AWS/2000_Well_Architected_Framework/#mechanical-sympathy","title":"Mechanical sympathy","text":""},{"location":"AWS/2000_Well_Architected_Framework/#best-practices-for-performance-efficiency-selection-review-monitoring-tradeoffs","title":"Best Practices for Performance Efficiency: Selection, Review, Monitoring, Tradeoffs","text":""},{"location":"AWS/2000_Well_Architected_Framework/#performance-efficiency-selection","title":"Performance Efficiency: Selection","text":""},{"location":"AWS/2000_Well_Architected_Framework/#performance-efficiency-review","title":"Performance Efficiency: Review","text":""},{"location":"AWS/2000_Well_Architected_Framework/#performance-efficiency-monitoring","title":"Performance Efficiency: Monitoring","text":""},{"location":"AWS/2000_Well_Architected_Framework/#performance-efficiency-tradeoffs","title":"Performance Efficiency: Tradeoffs","text":""},{"location":"AWS/2000_Well_Architected_Framework/#todo-finish-performance-efficiency-and-cost-optimization-once-i-learn-azure-better","title":"TODO: Finish Performance Efficiency and Cost Optimization once I learn Azure better","text":"<p>There is a strong short-term need for Azure support in CloudOps.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#aws-well-architected-tool","title":"AWS Well-Architected Tool","text":"<p>Checks an existing workload for known design issues.</p>"},{"location":"AWS/2000_Well_Architected_Framework/#conclusion","title":"Conclusion","text":"<p>The AWS Well-Architected Framework provides architectural best practices across the \ufb01ve pillars for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. The Framework provides a set of questions that allows you to review an existing or proposed architecture. It also provides a set of AWS best practices for each pillar. Using the Framework in your architecture will help you produce stable and efficient systems, which allow you to focus on your functional requirements. </p>"},{"location":"DevOps/DevOps_Cloud/","title":"Overview","text":""},{"location":"DevOps/DevOps_Cloud/#devops-model-defined","title":"DevOps Model Defined","text":"<p>DevOps is the combination of cultural philosophies, practices, and tools that increases an organization\u2019s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.</p>"},{"location":"DevOps/DevOps_Cloud/#how-devops-works","title":"How DevOps Works?","text":"<p>Under a DevOps model, development and operations teams are no longer \u201csiloed.\u201d Sometimes, these two teams are merged into a single team where the engineers work across the entire application lifecycle, from development and test to deployment to operations, and develop a range of skills not limited to a single function.</p> <p>In some DevOps models, quality assurance and security teams may also become more tightly integrated with development and operations and throughout the application lifecycle. When security is the focus of everyone on a DevOps team, this is sometimes referred to as DevSecOps.</p> <p>These teams use practices to automate processes that historically have been manual and slow. They use a technology stack and tooling which help them operate and evolve applications quickly and reliably. These tools also help engineers independently accomplish tasks (for example, deploying code or provisioning infrastructure) that normally would have required help from other teams, and this further increases a team\u2019s velocity.</p>"},{"location":"DevOps/DevOps_Cloud/#benefits-of-devops","title":"Benefits of DevOps","text":""},{"location":"DevOps/DevOps_Cloud/#speed","title":"Speed","text":"<p>Move at high velocity so you can innovate for customers faster, adapt to changing markets better, and grow more efficient at driving business results. The DevOps model enables your developers and operations teams to achieve these results. For example, microservices and continuous delivery let teams take ownership of services and then release updates to them quicker.</p>"},{"location":"DevOps/DevOps_Cloud/#rapid-delivery","title":"Rapid Delivery","text":"<p>Increase the frequency and pace of releases so you can innovate and improve your product faster. The quicker you can release new features and fix bugs, the faster you can respond to your customers\u2019 needs and build competitive advantage. Continuous integration and continuous delivery are practices that automate the software release process, from build to deploy.</p>"},{"location":"DevOps/DevOps_Cloud/#reliability","title":"Reliability","text":"<p>Ensure the quality of application updates and infrastructure changes so you can reliably deliver at a more rapid pace while maintaining a positive experience for end users. Use practices like continuous integration and continuous delivery to test that each change is functional and safe. Monitoring and logging practices help you stay informed of performance in real-time.</p>"},{"location":"DevOps/DevOps_Cloud/#scale","title":"Scale","text":"<p>Operate and manage your infrastructure and development processes at scale. Automation and consistency help you manage complex or changing systems efficiently and with reduced risk. For example, infrastructure as code helps you manage your development, testing, and production environments in a repeatable and more efficient manner.</p>"},{"location":"DevOps/DevOps_Cloud/#improved-collaboration","title":"Improved Collaboration","text":"<p>Build more effective teams under a DevOps cultural model, which emphasizes values such as ownership and accountability. Developers and operations teams collaborate closely, share many responsibilities, and combine their workflows. This reduces inefficiencies and saves time (e.g. reduced handover periods between developers and operations, writing code that takes into account the environment in which it is run).</p>"},{"location":"DevOps/DevOps_Cloud/#security","title":"Security","text":"<p>Move quickly while retaining control and preserving compliance. You can adopt a DevOps model without sacrificing security by using automated compliance policies, fine-grained controls, and configuration management techniques. For example, using infrastructure as code and policy as code, you can define and then track compliance at scale.</p>"},{"location":"DevOps/DevOps_Cloud/#why-devops-matters","title":"Why DevOps Matters","text":"<p>Software and the Internet have transformed the world and its industries, from shopping to entertainment to banking. Software no longer merely supports a business; rather it becomes an integral component of every part of a business. Companies interact with their customers through software delivered as online services or applications and on all sorts of devices. They also use software to increase operational efficiencies by transforming every part of the value chain, such as logistics, communications, and operations. In a similar way that physical goods companies transformed how they design, build, and deliver products using industrial automation throughout the 20<sup>th</sup> century, companies in today\u2019s world must transform how they build and deliver software.</p>"},{"location":"DevOps/DevOps_Cloud/#how-to-adopt-a-devops-model","title":"How to Adopt a DevOps Model","text":""},{"location":"DevOps/DevOps_Cloud/#devops-cultural-philosophy","title":"DevOps Cultural Philosophy","text":"<p>Transitioning to DevOps requires a change in culture and mindset. At its simplest, DevOps is about removing the barriers between two traditionally siloed teams, development and operations. In some organizations, there may not even be separate development and operations teams; engineers may do both. With DevOps, the two teams work together to optimize both the productivity of developers and the reliability of operations. They strive to communicate frequently, increase efficiencies, and improve the quality of services they provide to customers. They take full ownership for their services, often beyond where their stated roles or titles have traditionally been scoped by thinking about the end customer\u2019s needs and how they can contribute to solving those needs. Quality assurance and security teams may also become tightly integrated with these teams. Organizations using a DevOps model, regardless of their organizational structure, have teams that view the entire development and infrastructure lifecycle as part of their responsibilities.</p>"},{"location":"DevOps/DevOps_Cloud/#devops-practices-explained","title":"DevOps Practices Explained","text":"<p>There are a few key practices that help organizations innovate faster through automating and streamlining the software development and infrastructure management processes. Most of these practices are accomplished with proper tooling.</p> <p>One fundamental practice is to perform very frequent but small updates. This is how organizations innovate faster for their customers. These updates are usually more incremental in nature than the occasional updates performed under traditional release practices. Frequent but small updates make each deployment less risky. They help teams address bugs faster because teams can identify the last deployment that caused the error. Although the cadence and size of updates will vary, organizations using a DevOps model deploy updates much more often than organizations using traditional software development practices.</p> <p>Organizations might also use a microservices architecture to make their applications more flexible and enable quicker innovation. The microservices architecture decouples large, complex systems into simple, independent projects. Applications are broken into many individual components (services) with each service scoped to a single purpose or function and operated independently of its peer services and the application as a whole. This architecture reduces the coordination overhead of updating applications, and when each service is paired with small, agile teams who take ownership of each service, organizations can move more quickly.</p> <p>However, the combination of microservices and increased release frequency leads to significantly more deployments which can present operational challenges. Thus, DevOps practices like continuous integration and continuous delivery solve these issues and let organizations deliver rapidly in a safe and reliable manner. Infrastructure automation practices, like infrastructure as code and configuration management, help to keep computing resources elastic and responsive to frequent changes. In addition, the use of monitoring and logging helps engineers track the performance of applications and infrastructure so they can react quickly to problems.</p> <p>Together, these practices help organizations deliver faster, more reliable updates to their customers. Here is an overview of important DevOps practices.</p>"},{"location":"DevOps/DevOps_Cloud/#devops-practices","title":"DevOps Practices","text":""},{"location":"DevOps/DevOps_Cloud/#continuous-integration","title":"Continuous Integration","text":"<p>Continuous integration is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. The key goals of continuous integration are to find and address bugs quicker, improve software quality, and reduce the time it takes to validate and release new software updates.</p>"},{"location":"DevOps/DevOps_Cloud/#continuous-delivery","title":"Continuous Delivery","text":"<p>Continuous delivery is a software development practice where code changes are automatically built, tested, and prepared for a release to production. It expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage. When continuous delivery is implemented properly, developers will always have a deployment-ready build artifact that has passed through a standardized test process.</p>"},{"location":"DevOps/DevOps_Cloud/#microservices","title":"Microservices","text":"<p>The microservices architecture is a design approach to build a single application as a set of small services. Each service runs in its own process and communicates with other services through a well-defined interface using a lightweight mechanism, typically an HTTP-based application programming interface (API). Microservices are built around business capabilities; each service is scoped to a single purpose. You can use different frameworks or programming languages to write microservices and deploy them independently, as a single service, or as a group of services.</p>"},{"location":"DevOps/DevOps_Cloud/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>Infrastructure as code is a practice in which infrastructure is provisioned and managed using code and software development techniques, such as version control and continuous integration. The cloud\u2019s API-driven model enables developers and system administrators to interact with infrastructure programmatically, and at scale, instead of needing to manually set up and configure resources. Thus, engineers can interface with infrastructure using code-based tools and treat infrastructure in a manner similar to how they treat application code. Because they are defined by code, infrastructure and servers can quickly be deployed using standardized patterns, updated with the latest patches and versions, or duplicated in repeatable ways.</p>"},{"location":"DevOps/DevOps_Cloud/#configuration-management","title":"Configuration Management","text":"<p>Developers and system administrators use code to automate operating system and host configuration, operational tasks, and more. The use of code makes configuration changes repeatable and standardized. It frees developers and systems administrators from manually configuring operating systems, system applications, or server software.</p>"},{"location":"DevOps/DevOps_Cloud/#policy-as-code","title":"Policy as Code","text":"<p>With infrastructure and its configuration codified with the cloud, organizations can monitor and enforce compliance dynamically and at scale. Infrastructure that is described by code can thus be tracked, validated, and reconfigured in an automated way. This makes it easier for organizations to govern changes over resources and ensure that security measures are properly enforced in a distributed manner (e.g. information security or compliance with PCI-DSS or HIPAA). This allows teams within an organization to move at higher velocity since non-compliant resources can be automatically flagged for further investigation or even automatically brought back into compliance.</p>"},{"location":"DevOps/DevOps_Cloud/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>Organizations monitor metrics and logs to see how application and infrastructure performance impacts the experience of their product\u2019s end user. By capturing, categorizing, and then analyzing data and logs generated by applications and infrastructure, organizations understand how changes or updates impact users, shedding insights into the root causes of problems or unexpected changes. Active monitoring becomes increasingly important as services must be available 24/7 and as application and infrastructure update frequency increases. Creating alerts or performing real-time analysis of this data also helps organizations more proactively monitor their services.</p>"},{"location":"DevOps/DevOps_Cloud/#communication-and-collaboration","title":"Communication and Collaboration","text":"<p>Increased communication and collaboration in an organization is one of the key cultural aspects of DevOps. The use of DevOps tooling and automation of the software delivery process establishes collaboration by physically bringing together the workflows and responsibilities of development and operations. Building on top of that, these teams set strong cultural norms around information sharing and facilitating communication through the use of chat applications, issue or project tracking systems, and wikis. This helps speed up communication across developers, operations, and even other teams like marketing or sales, allowing all parts of the organization to align more closely on goals and projects.</p>"},{"location":"DevOps/DevOps_Cloud/#devops-tools","title":"DevOps Tools","text":"<p>The DevOps model relies on effective tooling to help teams rapidly and reliably deploy and innovate for their customers. These tools automate manual tasks, help teams manage complex environments at scale, and keep engineers in control of the high velocity that is enabled by DevOps. </p>"},{"location":"Kubernetes/Installing_Kubernetes/","title":"Installing Kubernetes cluster on unmanaged Cloud Platform VM's","text":"<p>We are going to install kubernetes cluster on the Cloud platforms and will deploy pod &amp; expose services to outside network. You can use the AzureCLI, if you are using Azure Cloud  to deploy the VM's &amp; Ansible scripts from my github  to install &amp; configure kubernetes.</p>"},{"location":"Kubernetes/Installing_Kubernetes/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>A Kubernetes master node with mininum of 2vCPU and 4GB Memory.</p> <ol> <li>Passwordless authentication to all nodes as root. (Remember to enble root login to yes in sshd_config).</li> </ol> </li> <li> <p>Two or Three worker nodes with at least 2 vCPUs and 8GB RAM each.</p> </li> <li> <p>Ansible installed on master. For installation instructions, follow the official Ansible installation documentation.</p> <ol> <li>Update the /etc/hosts file on all servers:</li> </ol> <pre><code>10.1.0.4 master\n10.1.0.5 worker1\n10.1.0.6 worker2\n</code></pre> </li> </ol>"},{"location":"Kubernetes/Installing_Kubernetes/#step-1-setting-up-the-workspace-directory-and-ansible-inventory-file-as-root-user","title":"Step 1 \u2014 Setting Up the Workspace Directory and Ansible Inventory file as root user","text":"<p>This will be on the master node we created.</p> <pre><code>mkdir ~/kube-cluster ; cd ~/kube-cluster\n</code></pre> <p>This directory will be our workspace and will contain all of our Ansible playbooks. It will also be the directory inside which you will run local commands.</p> <p>Create a file named <code>hosts</code> using nano or your favorite text editor and update below:</p> <pre><code>[masters]\nmaster ansible_host=10.1.0.4 ansible_user=root\n\n[workers]\nworker1 ansible_host=10.1.0.5 ansible_user=root\nworker2 ansible_host=10.1.0.6 ansible_user=root\n\n[all:vars]\nansible_python_interpreter=/usr/bin/python3\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#step-2-creating-a-non-root-user-ansible-on-all-remote-servers","title":"Step 2 \u2014 Creating a Non-Root User (ansible) on All Remote Servers","text":"<p>We will create a non-root user to execute kubectl commands.</p> <p>Important</p> <p>We will be using root account for cluster installation and all kubectl commands should be executed on master server as ansible user.</p> <pre><code>ansible-playbook -i hosts ~/kube-cluster/user_creation_initial.yml\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#step-3-installing-kubernetes-dependencies","title":"Step 3 \u2014 Installing Kubernetes &amp; Dependencies","text":"<pre><code>ansible-playbook -i hosts ~/kube-cluster/kube_dependencies_install.yml\n</code></pre> <p>(Current version of k8s is 1.21.0-00 change it as required)</p> <p>After execution, Docker, kubeadm, and kubelet will be installed on all of the remote servers. kubectl is not a required component and is only needed for executing cluster commands.</p>"},{"location":"Kubernetes/Installing_Kubernetes/#step-4-setting-up-the-master-node","title":"Step 4 \u2014 Setting Up the Master Node","text":"<pre><code>ansible-playbook -i hosts ~/kube-cluster/master_initial.yml \n</code></pre> <p>Execute below command as ansible user to verify the cluster status:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Output:</p> <pre><code>ansible@debian1:~$ kubectl get nodes\nNAME      STATUS   ROLES                  AGE   VERSION\ndebian1   Ready    control-plane,master   20m   v1.21.0\nansible@debian1:~$\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#step-5-setting-up-the-worker-nodes","title":"Step 5 \u2014 Setting Up the Worker Nodes","text":"<p>Caution</p> <p>Run below command as root user</p> <pre><code>ansible-playbook -i hosts ~/kube-cluster/join_worker_nodes.yml\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#step-6-verifying-the-cluster","title":"Step 6 \u2014 Verifying the Cluster","text":"<p>Now onwards we will be using ansible user for creating deployments and services.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Output:</p> <pre><code>ansible@debian1:~$ kubectl get nodes\nNAME      STATUS   ROLES                  AGE   VERSION\ndebian1   Ready    control-plane,master   24m   v1.21.0\ndebian2   Ready    &lt;none&gt;                 81s   v1.21.0\ndebian3   Ready    &lt;none&gt;                 81s   v1.21.0\nansible@debian1:~$\n</code></pre> <p>If all of your nodes have the value Ready for STATUS, it means that they\u2019re part of the cluster and ready to run workloads.</p> <p>Success</p> <p>You have successfully installed Kubernetes cluster.</p>"},{"location":"Kubernetes/Installing_Kubernetes/#step-7-deploying-applications-in-the-cluster","title":"Step 7 \u2014 Deploying Applications in the Cluster","text":"<p>Deployment scripts are here.</p>"},{"location":"Kubernetes/Installing_Kubernetes/#deployment","title":"Deployment","text":"<p>A deployment is a type of Kubernetes object that ensures there\u2019s always a specified number of pods running based on a defined template, even if the pod crashes during the cluster\u2019s lifetime. The deployment will create a pod with one container from the Docker registry\u2019s Nginx Docker Image.</p> <pre><code>kubectl create -f nginx-deploy.yml\n</code></pre> <p>Once the deployment is successful verify the status using below.</p> <pre><code>kubectl get deployment -l app=nginx-app\n</code></pre> <pre><code>ansible@master:~$ kubectl get deployment -l app=nginx-app\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           45m\nansible@master:~$\n</code></pre> <p>Deployment is competed with 3 replica set of nginx</p>"},{"location":"Kubernetes/Installing_Kubernetes/#services","title":"Services","text":"<p>Services are another type of Kubernetes object that expose cluster internal services to clients, both internal and external. They are also capable of load balancing requests to multiple pods, and are an integral component in Kubernetes, frequently interacting with other components.</p> <p>Next, run the following command to create a service named nginx-service that will expose the app to Public IP. It will do so through a NodePort, a scheme that will make the pod accessible through an arbitrary port on a worker public IP:</p> <pre><code>kubectl create -f nginx-service-np.yml\n</code></pre> <p>Output:</p> <pre><code>ansible@master:~$ kubectl get service -l app=nginx-app\nNAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nnginx-service-np   NodePort   10.104.48.148   &lt;none&gt;        80:31000/TCP   4m27s\nansible@master:~$\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#to-test-that-everything-is-working","title":"To test that everything is working","text":"<p>Check on which worker node the pods are running</p> <pre><code>kubectl get pods --output=wide\n</code></pre> <pre><code>ansible@master:~$  kubectl get pods --output=wide\nNAME                               READY   STATUS    RESTARTS   AGE    IP               NODE      NOMINATED NODE   READINESS GATES\nnginx-deployment-8bc69dc67-c62t7   1/1     Running   0          120m   10.244.235.131   worker1   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-8bc69dc67-crjbd   1/1     Running   0          120m   10.244.189.66    worker2   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-8bc69dc67-frsv5   1/1     Running   0          111m   10.244.189.67    worker2   &lt;none&gt;           &lt;none&gt;\nansible@master:~$\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#now-you-can-use-the-public-ip-of-any-worker-node-with-port-31000-to-get-the-default-nginx-page","title":"Now you can use the public IP of any worker node with port 31000 to get the default nginx page","text":"<p>Info</p> <p>NodePort is not recommended in Production Enviornment instead use LoadBalancer or configure ingress.</p>"},{"location":"Kubernetes/Installing_Kubernetes/#cleanup-the-resources","title":"Cleanup the resources","text":""},{"location":"Kubernetes/Installing_Kubernetes/#if-you-would-like-to-remove-the-nginx-application-first-delete-the-nginx-service-from-the-master-node","title":"If you would like to remove the Nginx application, first delete the nginx service from the master node","text":"<pre><code>kubectl delete -f nginx-service-np.yml\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#run-the-following-to-ensure-that-the-service-has-been-deleted","title":"Run the following to ensure that the service has been deleted","text":"<pre><code>kubectl get services\n</code></pre> <p>Output:</p> <pre><code>ansible@debian1:~$ kubectl get services\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   38m\nansible@debian1:~$\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#then-delete-the-deployment","title":"Then delete the deployment","text":"<pre><code>kubectl delete -f nginx-deploy.yml\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#run-the-kubectl-get-services-again-to-confirm-it-worked","title":"Run the <code>kubectl get services</code> again to confirm it worked","text":"<p>Output:</p> <pre><code>ansible@debian1:~$ kubectl get deployments\nNo resources found in default namespace.\nansible@debian1:~$\n</code></pre>"},{"location":"Kubernetes/Installing_Kubernetes/#to-delete-all-the-resources-created-in-azure","title":"To delete all the resources created in Azure","text":"<p>If you used the scripts. We created all the resources in one single Resource Group. By deleting the RG,  all the resources created inside the RG will be deleted.</p> <p>Use <code>Azure_delete_resourcegroup.ps1 from</code> AzureCLI</p>"},{"location":"Kubernetes/Overview_Kubernetes/","title":"Kubernetes","text":""},{"location":"Kubernetes/Overview_Kubernetes/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p> <p>The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the \"K\" and the \"s\". Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google's experience running production workloads at scale with best-of-breed ideas and practices from the community.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#kubernetes-components","title":"Kubernetes Components","text":"<p>When you deploy Kubernetes, you get a cluster.</p> <p>A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node.</p> <p>The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability.</p> <p>This document outlines the various components you need to have a complete and working Kubernetes cluster.</p> <p>Here's the diagram of a Kubernetes cluster with all the components tied together.</p> <p></p>"},{"location":"Kubernetes/Overview_Kubernetes/#control-plane-components","title":"Control Plane Components","text":"<p>The control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new pod when a deployment's replicas field is unsatisfied).</p> <p>Control plane components can be run on any machine in the cluster. However, for simplicity, set up scripts typically start all control plane components on the same machine, and do not run user containers on this machine. See Creating Highly Available clusters with kubeadm for an example control plane setup that runs across multiple VMs.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#kube-apiserver","title":"kube-apiserver","text":"<p>The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.</p> <p>The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontally\u2014that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#etcd","title":"etcd","text":"<p>Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p> <p>If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for those data.</p> <p>You can find in-depth information about etcd in the official documentation.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#kube-scheduler","title":"kube-scheduler","text":"<p>Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.</p> <p>Factors taken into account for scheduling decisions include: individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#kube-controller-manager","title":"kube-controller-manager","text":"<p>Control Plane component that runs controller processes.</p> <p>Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.</p> <p>Some types of these controllers are:</p> <ul> <li>Node controller: Responsible for noticing and responding when nodes go down.</li> <li> <p>Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.</p> </li> <li> <p>Endpoints controller: Populates the Endpoints object (that is, joins Services &amp; Pods).</p> </li> <li> <p>Service Account &amp; Token controllers: Create default accounts and API access tokens for new namespaces.</p> </li> </ul>"},{"location":"Kubernetes/Overview_Kubernetes/#cloud-controller-manager","title":"cloud-controller-manager","text":"<p>A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.</p> <p>The cloud-controller-manager only runs controllers that are specific to your cloud provider. If you are running Kubernetes on your own premises, or in a learning environment inside your own PC, the cluster does not have a cloud controller manager.</p> <p>As with the kube-controller-manager, the cloud-controller-manager combines several logically independent control loops into a single binary that you run as a single process. You can scale horizontally (run more than one copy) to improve performance or to help tolerate failures.</p> <p>The following controllers can have cloud provider dependencies:</p> <ul> <li>Node controller: For checking the cloud provider to determine if a node has been deleted in        the cloud after it stops responding</li> <li>Route controller: For setting up routes in the underlying cloud infrastructure</li> <li>Service controller: For creating, updating and deleting cloud provider load balancers</li> </ul>"},{"location":"Kubernetes/Overview_Kubernetes/#node-components","title":"Node Components","text":"<p>Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#kubelet","title":"kubelet","text":"<p>An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.</p> <p>The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#kube-proxy","title":"kube-proxy","text":"<p>kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.</p> <p>kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.</p> <p>kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#container-runtime","title":"Container runtime","text":"<p>The container runtime is the software that is responsible for running containers.</p> <p>Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface).</p>"},{"location":"Kubernetes/Overview_Kubernetes/#addons","title":"Addons","text":"<p>Addons use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features. Because these are providing cluster-level features, namespaced resources for addons belong within the <code>kube-system</code> namespace.</p> <p>Selected addons are described below; for an extended list of available addons, please see Addons.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#dns","title":"DNS","text":"<p>While the other addons are not strictly required, all Kubernetes clusters should have cluster DNS, as many examples rely on it.</p> <p>Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves ## DNS records for Kubernetes services.</p> <p>Containers started by Kubernetes automatically include this DNS server in their DNS searches.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#web-ui-dashboard","title":"Web UI (Dashboard)","text":"<p>Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself.</p>"},{"location":"Kubernetes/Overview_Kubernetes/#container-resource-monitoring","title":"Container Resource Monitoring","text":"<p>Container Resource Monitoring records generic time-series metrics about containers in a central database, and provides a UI for browsing that data</p>"},{"location":"Kubernetes/Overview_Kubernetes/#cluster-level-logging","title":"Cluster-level Logging","text":"<p>cluster-level logging mechanism is responsible for saving container logs to a central log store with search/browsing interface.</p>"},{"location":"aws/","title":"AWS","text":""},{"location":"devops/","title":"DevOps","text":""},{"location":"kubernetes/","title":"Kubernetes","text":""}]}